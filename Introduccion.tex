% !TeX spellcheck = es_ES

% Cada capítulo de la memoria de TFG comienza con \chapter{TÍTULO DEL CAPÍTULO}, tal y como requiere la normativa de la EPSJ
\chapter{INTRODUCCIÓN}  

Este capítulo describe cómo emplear la plantilla \LaTeX~ para generar una memoria de TFG ajustada a las normas establecidas por la \textit{Escuela Politécnica Superior de Jaén} (EPSJ). La plantilla está diseñada para su uso con \verb*|pdflatex|, el compilador de \LaTeX~ más popular para generar documentos en formato \verb*|PDF|.

La estructura de esta plantilla se corresponde con la requerida por la EPSJ para los TFG de tipo teórico o experimental. Estos no son los más habituales en el \textit{Grado en Ingeniería Informática}, cuyo tipo más común es el \textit{Proyecto de Ingeniería}. Puedes usar esta plantilla para generar una memoria de ese tipo modificando los nombres y contenidos de los capítulos y secciones.

Este capítulo de introducción se usa en la plantilla para describir cómo escribir la memoria usando \LaTeX~, por lo que tendrás que sustituir este contenido por el corresponda a tu memoria. Para \textit{compilar} los fuentes \LaTeX~ y obtener el correspondiente \verb*|PDF| necesitarás una distribución local de \LaTeX~, como puede ser TeX Live (\url{en.wikipedia.org/wiki/TeX\_Live}) o MiKTeX (\url{en.wikipedia.org/wiki/MiKTeX}), o bien trabajar con un editor \LaTeX~ en la nube como puede ser Overleaf (\url{https://overleaf.com}).


El presente Proyecto de Fin de Grado aborda una de las problemáticas más acuciantes en el ámbito de la ciberseguridad: la detección proactiva de intrusiones en redes informáticas. En un panorama digital en constante evolución, donde las amenazas son cada vez más sofisticadas y persistentes, la capacidad de identificar y neutralizar actividades maliciosas en tiempo real se ha convertido en un pilar fundamental para la protección de la información y la infraestructura crítica. Tradicionalmente, los Sistemas de Detección de Intrusiones (IDS) se basaban en firmas o reglas predefinidas, un enfoque que, si bien eficaz contra amenazas conocidas, presenta limitaciones intrínsecas ante nuevos ataques o variaciones de los existentes.

Este trabajo propone el desarrollo de un Sistema de Detección de Intrusiones (IDS) innovador, que fusiona la captura y el monitoreo de tráfico de red en tiempo real con la potencia de las técnicas de aprendizaje automático (Machine Learning). El objetivo principal es la creación de un modelo de clasificación robusto y preciso, capaz de discernir de forma autónoma si un determinado flujo de tráfico de red es benigno (legítimo) o malicioso (indicativo de una intrusión o ataque). Esta aproximación busca superar las deficiencias de los sistemas basados en firmas, ofreciendo una mayor adaptabilidad y capacidad para detectar anomalías y comportamientos nunca antes vistos.

Para lograr este cometido, se hará uso de diversas tecnologías y metodologías de vanguardia. La fase de captura y preprocesamiento de datos de red se implementará mediante una aplicación diseñada específicamente para tal fin, garantizando la obtención de información relevante y en el formato adecuado para su análisis. Posteriormente, esta información será alimentada a un modelo de Machine Learning, concretamente un algoritmo de Random Forest (Bosques Aleatorios), seleccionado por su probada eficacia, robustez y capacidad para manejar grandes volúmenes de datos con alta dimensionalidad, características esenciales en el análisis de tráfico de red.

La estructura de este primer capítulo sentará las bases conceptuales y técnicas de todo el proyecto. A través de una descripción detallada de los componentes clave y la visión general del sistema propuesto, se facilitará la comprensión de los antecedentes del trabajo, los cuales serán explorados en profundidad en el segundo capítulo de esta memoria. Este enfoque metodológico permite establecer un marco claro para la posterior justificación, diseño e implementación de la solución planteada, enfatizando la relevancia y el impacto potencial de un IDS basado en aprendizaje automático en el contexto actual de la ciberseguridad.

\section{Fundamentos}

En este capítulo se establecerán las bases teóricas y conceptuales fundamentales que sustentan el análisis, diseño e implementación del sistema de detección de intrusiones propuesto. Se abordará en profundidad el paradigma de los Sistemas de Detección de Intrusiones, las bases del aprendizaje automático como disciplina, y se hará un énfasis particular en el comprensión del algoritmo de Random Forest, pieza clave de la solución desarrollada. Finalmente, se detallarán las métricas esenciales para la evaluación rigurosa del rendimiento de modelos de clasificación en el contexto de la ciberseguridad. Se comenzará explicando el concepto de \textbf{ciberseguridad y amenazas en redes}.


Una memoria de TFG es similar a un libro y, por extensión, puede llegar a ser incluso más extensa. Por ello su contenido se estructura en varios niveles, facilitando así una numeración jerárquica que puedes ver reflejada en el índice que precede a esta introducción. 

Los niveles de uso habitual para generar esa jerarquía son:

\begin{itemize}
    \item 1. Capítulo
    \begin{itemize}
        \item 1.1. Sección
        \begin{itemize}
            \item 1.1.1. Subsección
            \begin{itemize}
                \item 1.1.1.1. Subsubsección
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{itemize}

No se recomienda tener más de estos cuatro niveles, aunque podrían usarse otros para dar mayor profundidad a la jerarquía.

\subsection{Ciberseguridad y amenazas en redes}

La seguridad de la información constituye un pilar fundamental en la sociedad digital contemporánea, donde la interconexión global y la dependencia de los sistemas informáticos son crecientes. La protección de los activos digitales se articula en torno a los principios de confidencialidad, integridad y disponibilidad (CID). La confidencialidad garantiza que la información sea accesible únicamente por entidades autorizadas; la integridad asegura que la información no ha sido alterada de forma no autorizada; y la disponibilidad se refiere a la capacidad de los usuarios autorizados para acceder a la información y los sistemas cuando sea necesario. Cualquier evento que comprometa uno o más de estos principios se clasifica como una amenaza, la cual, al explotar una vulnerabilidad existente, puede materializarse en un riesgo para la organización.

El panorama de amenazas en redes informáticas evoluciona constantemente, presentando un desafío dinámico para la ciberseguridad. Si bien históricamente los ataques se centraban en la explotación de vulnerabilidades conocidas o errores de configuración, la sofisticación actual se manifiesta en técnicas de evasión más complejas y la emergencia de amenazas persistentes avanzadas (APT). Entre los tipos de ataques más prevalentes que un sistema de detección de intrusiones debe ser capaz de identificar se encuentran:
\begin{itemize}

    \item\textbf{Ataques de Denegación de Servicio (DoS/DDoS)}: Dirigidos a agotar los recursos de un sistema o red, impidiendo el acceso a servicios legítimos.
    
    \item\textbf{Escaneo de Puertos y Reconocimiento}: Fases preliminares donde un atacante explora una red en busca de puntos débiles o servicios abiertos.
    
    \item\textbf{Ataques de Fuerza Bruta}: Intentos sistemáticos y repetitivos para adivinar credenciales de acceso o claves de cifrado.
    
    \item\textbf{Malware (Software Malicioso)}: Incluye virus, troyanos, ransomware y spyware, diseñados con propósitos destructivos, de espionaje o de control remoto.
    
    \item\textbf{Explotación de Vulnerabilidades}: Aprovechamiento de fallos de diseño o implementación en software y hardware para obtener acceso no autorizado o ejecutar código malicioso.
    La capacidad de identificar estos y otros comportamientos anómalos es crucial para mantener la postura de seguridad de una infraestructura de red.

\end{itemize}
La memoria se generará a partir de un documento principal al que llamaremos habitualmente \texttt{main.tex}. Este contendrá los datos necesarios para generar la portada y páginas iniciales de la memoria: nombre del estudiante, nombres de los tutores, título del TFG, fecha en que se deposita, etc.

En el documento principal existirán tantos comandos \verb*|\input{archivo.tex}| como capítulos existan, a fin de importar su contenido y generar así el \verb*|PDF| completo.

\subsection{Sistema de detección de instrusiones}\label{Sec.Capitulos}
La memoria se estructura en varios capítulos, en principio esta plantilla tiene tantos como apartados indica la normativa de la EPSJ, si bien existe cierta flexibilidad a la hora de agregar otros que se consideren necesarios.

El fuente \LaTeX~ para cada capítulo se alojará en un archivo independiente, con extensión \texttt{.tex}, que será incluido en el documento principal con el comando \verb*|\input|. La primera línea del capítulo será:

\begin{verbatim}
\chapter{TÍTULO DEL CAPÍTULO}
\end{verbatim}

Observa que el título se escribe completamente en mayúsculas. La numeración de los capítulos es automática, por lo que no debes preocuparte por este detalle.

En respuesta a la creciente complejidad del panorama de amenazas, los Sistemas de Detección de Intrusiones (IDS) han emergido como componentes esenciales de una estrategia de defensa en profundidad. Un IDS puede definirse como una aplicación de seguridad que monitoriza el tráfico de red o la actividad de un sistema con el fin de identificar patrones o comportamientos indicativos de una intrusión o una violación de políticas de seguridad. Su función principal no es bloquear el ataque (labor de un IPS o firewall), sino generar alertas que permitan a los administradores de seguridad tomar las medidas correctivas oportunas.

La clasificación de los IDS se realiza habitualmente atendiendo a su método de detección y a su ubicación en la infraestructura:
\begin{itemize}
    
    \item\textbf{IDS Basados en Firmas (Signature-based IDS - SIDS)}: Estos sistemas operan mediante la comparación del tráfico de red o eventos del sistema con una base de datos de firmas predefinidas, las cuales corresponden a patrones conocidos de ataques. Su principal ventaja reside en una alta precisión en la detección de ataques ya identificados y catalogados, con una baja tasa de falsos positivos en esos escenarios. No obstante, su limitación inherente radica en la incapacidad para detectar ataques novedosos o variantes polimórficas para las cuales no existe una firma en su base de datos, lo que exige una constante actualización de las mismas.
    
    \item\textbf{IDS Basados en Anomalías (Anomaly-based IDS - AVIDS)}: A diferencia de los SIDS, los AVIDS no dependen de firmas de ataques conocidos. En su lugar, construyen un perfil de comportamiento "normal" de la red, los usuarios o las aplicaciones. Cualquier desviación significativa de este perfil es considerada una anomalía y, por ende, una posible intrusión. La principal fortaleza de los AVIDS es su capacidad para detectar ataques "zero-day" (desconocidos previamente) y ataques sutiles que no encajan en patrones preestablecidos. Sin embargo, su principal desafío es la mayor propensión a generar falsos positivos, ya que cualquier cambio en el comportamiento normal (como la introducción de un nuevo servicio o una carga de tráfico inusual pero legítima) puede ser erróneamente clasificado como un ataque, requiriendo un ajuste continuo y afinado.
    
\end{itemize}

En cuanto a su ubicación, se distinguen los Network-based IDS (NIDS), que analizan el tráfico de red en puntos estratégicos de la infraestructura sin depender de los hosts individuales, y los Host-based IDS (HIDS), que monitorizan la actividad interna de un sistema operativo o aplicación específica. El sistema desarrollado en este proyecto se enmarca dentro de la categoría de NIDS, enfocándose en el análisis del tráfico de red.

\subsection{Clasificación}

Un capítulo se dividirá en secciones, cada una de las cuales se inicia con el siguiente comando:

\begin{verbatim}
\section{Título de la sección}
\end{verbatim}

Es recomendable planificar las secciones en que se dividirá un capítulo antes de comenzar a escribirlo. Debemos pensar qué queremos contar en el capítulo y cómo estructurar ese contenido en partes cohesionadas, cada una de las cuales daría lugar a una sección.

La clasificación es una de las principales tareas dentro de la minería de datos.
Como se expresó previamente, el objetivo de esta tarea es predecir una etiqueta categórica para cada ejemplo de un conjunto de datos a partir de sus atributos conocidos \cite{hastie2009elements}.

Existen diversos tipos de clasificación. Entre los más destacados, podemos encontrar:

\begin{itemize}
    \item\textbf{Clasificación binaria}: en este problema concreto de clasificación, es necesario determinar a qué clase pertenece cada ejemplo de los datos, pero únicamente hay dos clases a elegir. Se suelen usar para problemas de carácter binario, como si una persona está sana o enferma o, en el contexto de esta memoria, si una conexión determinada es benigna o un ataque.
    
    \item\textbf{Clasificación multiclase}: al contrario que en el caso previo, se debe determinar la clase de cada ejemplo de entre un conjunto de clases de tamaño superior a dos. En este TFG, también se hará uso de la clasificación multiclase para tratar de determinar el tipo de ataque de una conexión que se sabe maliciosa.
    
    \item\textbf{Clasificación multietiqueta}: un caso particular dentro de la clasificación multiclase es la clasificación multietiqueta, ya que, en lugar de asignar una sola clase a cada ejemplo del conjunto de datos, se le deben asignar un conjunto de clases en función a las características de los datos, a menudo siendo posible que cada elemento tenga un número de clases asociadas distinto.

\end{itemize}

\subsection{Descubrimiento de conocimiento en bases de datos}


Las secciones de cierta extensión se dividirán en subsecciones. De igual forma, una subsección con mucho contenido se dividiría en subsecciones. Los comandos para generar estos niveles son:

\begin{verbatim}
\subsection{Título de la subsección}

\subsubsection{Título de la subsubsección}
\end{verbatim}

Aunque obviamente es algo subjetivo, en general no es deseable tener varias páginas seguidas de texto sin ninguna división que permita tener un punto de referencia.

El Descubrimiento de Conocimiento en Bases de Datos (KDD) es un proceso sistemático y iterativo, no trivial, diseñado para la extracción de patrones válidos, novedosos, potencialmente útiles y comprensibles a partir de grandes volúmenes de datos. Representa la filosofía subyacente a la transformación de datos crudos en inteligencia accionable, siendo crucial en dominios donde la toma de decisiones basada en evidencia es primordial, como la ciberseguridad. En el contexto de un Sistema de Detección de Intrusiones (IDS) basado en aprendizaje automático, el KDD proporciona el marco metodológico para derivar el "conocimiento" que permite al sistema identificar comportamientos anómalos o maliciosos en el tráfico de red.

El proceso de KDD se articula a través de una secuencia de etapas interdependientes:

\textbf{Fases del Proceso KDD}
\begin{enumerate}

    \item\textbf{Selección}: Esta etapa inicial define y adquiere los datos relevantes del dominio de aplicación. Para un IDS, esto implica la selección de datasets de tráfico de red, como el CIC-IDS2018, que contengan una representación adecuada de comportamientos tanto benignos como maliciosos. La exhaustividad y representatividad de esta selección son fundamentales para la generalizabilidad del modelo resultante.
    
    \item\textbf{Preprocesamiento}: Considerada a menudo la fase más laboriosa y crítica, el preprocesamiento aborda la preparación de los datos brutos. Los datos de red, por su naturaleza, suelen ser ruidosos, incompletos, o inconsistentes. Esta etapa involucra:
    \begin{itemize}

        \item\textbf{Limpieza de Datos}: Eliminación o corrección de datos erróneos, duplicados, inconsistencias y manejo de valores ausentes.
        
        \item\textbf{Normalización/Estandarización}: Ajuste de las escalas de las características numéricas para evitar que aquellas con rangos de valores más amplios dominen en el análisis.
        
        \item\textbf{Transformación}: Conversión de datos a formatos adecuados para la minería. En el caso de tráfico de red, esto incluye la agregación de paquetes individuales en flujos de red y la extracción de características de esos flujos (ej., duración, número de paquetes, banderas TCP), elementos esenciales para el aprendizaje automático.
          
    \end{itemize}
    \item\textbf{Transformación}: Aunque intrínsecamente ligada al preprocesamiento, esta fase se centra en refinar la representación de los datos para la minería. La ingeniería de características es su componente clave, donde el conocimiento experto del dominio de red se aplica para derivar atributos más complejos y discriminatorios (ej., ratios de bytes por paquete, entropía de los puertos destino). El objetivo es crear un conjunto de características que optimicen la capacidad del algoritmo de Machine Learning para identificar patrones de intrusión.
    
    \item\textbf{Minería de Datos (Data Mining)}: Este es el corazón del proceso KDD, donde se aplican algoritmos computacionales para descubrir patrones ocultos, asociaciones, cambios significativos, desviaciones o estructuras significativas en los datos. No es simplemente una técnica, sino un conjunto de enfoques, tareas y técnicas aplicadas a los datos ya preprocesados:
    \begin{itemize}

    \item\textbf{Enfoques de Minería de Datos}:
        \begin{itemize}
        
            \item\textbf{Aprendizaje Supervisado}: Como se detalla en el punto de Transformación, implica el uso de datos etiquetados para predecir un resultado. La detección de intrusiones es, en esencia, un problema de clasificación supervisada, donde se predice si un flujo es benigno o malicioso.
            
            \item\textbf{Aprendizaje No Supervisado}: Se utiliza para encontrar estructuras o patrones en datos sin etiquetas previas (ej., clustering para agrupar tráficos similares, o detección de anomalías para identificar comportamientos que se desvían de lo normal sin una etiqueta de "ataque" explícita). Aunque tu proyecto se centra en lo supervisado, estos enfoques complementarios son relevantes en KDD.
            
            \item\textbf{Aprendizaje Semi-supervisado}: Combina datos etiquetados y no etiquetados, útil en escenarios donde el etiquetado es costoso.
            
        \end{itemize}

    \item\textbf{Tareas Típicas de Minería de Datos}:
        \begin{itemize}
        
            \item\textbf{Clasificación}: Asignar elementos a categorías predefinidas (ej., "ataque" o "normal"). Esta es la tarea principal de tu IDS.
            
            \item\textbf{Regresión}: Predecir un valor numérico continuo.
            
            \item\textbf{Agrupación (Clustering)}: Dividir un conjunto de datos en grupos (clusters) de elementos similares.
            
            \item\textbf{Asociación}: Descubrir reglas que describen relaciones entre elementos.
            
            \item\textbf{Detección de Anomalías/Outliers}: Identificar patrones que no se ajustan a un comportamiento esperado.
              
        \end{itemize}
    \item\textbf{Técnicas Aplicadas en este Proyecto}: En este trabajo, la fase de minería de datos se concreta en la aplicación del algoritmo Random Forest para la tarea de clasificación. Su capacidad para manejar un gran número de características y su robustez ante el ruido lo hacen idóneo para los complejos datasets de tráfico de red.
    \end{itemize}
    
    \item\textbf{Evaluación y Presentación}: La etapa final valida la significancia y utilidad de los patrones descubiertos. Los modelos de clasificación se evalúan utilizando métricas específicas (precisión, sensibilidad, F1-score, curva ROC) en un conjunto de datos de prueba independiente, asegurando que el conocimiento extraído sea generalizable y no un artefacto del entrenamiento. La interpretabilidad del modelo, aunque un desafío, es un objetivo deseable que permite a los analistas de seguridad comprender la razón de una detección. Finalmente, la presentación del conocimiento se realiza a través de informes, visualizaciones (como el dashboard de tu aplicación), que comunican de manera efectiva los insights a los usuarios finales.

\end{enumerate}
\subsection{Machine Learning}

Tras los comandos \verb|\chapter|, \verb|\section|, \verb|\subsection| y \verb|\subsection| dispondremos los párrafos de texto con las descripciones y explicaciones que procedan. Es recomendable tener al menos un párrafo de texto entre dos comandos sucesivos de los anteriores, evitando así tener el inicio de una sección justo tras el inicio del capítulo, de una subsección inmediatamente tras la sección, etc.

Los párrafos de texto en \LaTeX~ se separan unos de otros con una línea en blanco. El contenido de un párrafo se escribe todo seguido, sin pulsar nunca \textbf{Intro/Enter}. Al compilar se ajustará el texto automáticamente, introduciendo guiones donde sea necesario.

El aprendizaje automático (Machine Learning - ML), una rama de la inteligencia artificial, ha transformado diversas disciplinas al dotar a los sistemas de la capacidad de aprender de datos y mejorar su rendimiento con la experiencia, sin ser programados explícitamente para cada tarea. En el contexto de la ciberseguridad, y específicamente en la detección de intrusiones, el ML ofrece un enfoque adaptativo que puede identificar patrones complejos en grandes volúmenes de tráfico de red, superando las limitaciones de los métodos tradicionales basados en reglas fijas.

El ciclo de vida de un modelo de Machine Learning generalmente comprende las siguientes fases interdependientes:

\begin{itemize}
    
    \item\textbf{Recopilación y Preprocesamiento de Datos}: Etapa crucial que implica la obtención, limpieza, transformación y estandarización de los datos. La calidad de los datos de entrada es determinante para el éxito del modelo.
    
    \item\textbf{Ingeniería de Características (Feature Engineering)}: Proceso de seleccionar, crear o transformar variables (features) a partir de los datos brutos que mejor representen la información subyacente y sean más relevantes para el problema de predicción.
    
    \item\textbf{Selección y Entrenamiento del Modelo}: Consiste en elegir el algoritmo de ML más adecuado y ajustar sus parámetros utilizando un subconjunto de datos (conjunto de entrenamiento) para que aprenda los patrones deseados.
    
    \item\textbf{Evaluación y Optimización del Modelo}: Se mide el rendimiento del modelo con un conjunto de datos no visto (conjunto de validación o prueba) y se realizan ajustes de hiperparámetros para mejorar su eficacia.
    
    \item\textbf{Despliegue y Monitorización}: Una vez validado, el modelo se integra en un entorno de producción para realizar predicciones en tiempo real, siendo fundamental un monitoreo continuo de su desempeño.

\end{itemize}

Dentro del ML, el aprendizaje supervisado es el paradigma central para la detección de intrusiones basada en clasificación. En este enfoque, el modelo aprende de un conjunto de datos que incluye tanto las características de entrada (ej., métricas del tráfico de red) como sus correspondientes etiquetas de salida (ej., "tráfico normal" o "ataque"). El objetivo es que el modelo infiera una función que mapee las entradas a las salidas, permitiendo clasificar nuevas instancias de datos sin etiquetas. Esta investigación se inscribe en este paradigma, buscando clasificar el tráfico de red como benigno o malicioso.

\subsubsection{Preprocesamiento de datos para modelos de Machine Learning}

La efectividad de cualquier modelo de aprendizaje automático depende críticamente de la calidad y el formato de los datos de entrada. En el contexto de la detección de intrusiones, los datos brutos de tráfico de red, capturados en formato de paquetes, no son directamente utilizables por los algoritmos de Machine Learning. El preprocesamiento de datos es, por tanto, una fase indispensable que transforma estos paquetes en un formato estructurado y significativo para el análisis.

Las etapas fundamentales del preprocesamiento en un IDS basado en ML incluyen:
\begin{itemize}
   
    \item\textbf{Extracción y Reconstrucción de Flujos de Red}: Los algoritmos de ML operan sobre "instancias" o "muestras", que en el contexto de la red, son típicamente flujos. Un flujo se define como una secuencia de paquetes que comparten una tupla común de cinco elementos: dirección IP de origen, puerto de origen, dirección IP de destino, puerto de destino y protocolo de transporte. La reconstrucción de flujos a partir de paquetes individuales permite contextualizar la información y calcular características que abarcan la duración total de la conexión.
    
     \item\textbf{Generación de Características (Feature Engineering)}: Una vez reconstruidos los flujos, se extraen o derivan diversas características numéricas y categóricas que describen el comportamiento del flujo. Estas características deben ser consistentes con las utilizadas durante la fase de entrenamiento del modelo. Ejemplos de características incluyen:
    \begin{itemize}
         \item\textbf{Métricas de Volumen}: Número total de paquetes, bytes transmitidos (bidireccional), duración del flujo.
        
         \item\textbf{Métricas de Tasa}: Paquetes por segundo, bytes por segundo.
        
         \item\textbf{Características del Protocolo}: Banderas TCP (SYN, ACK, FIN, RST, PSH, URG), tipo de protocolo (TCP, UDP, ICMP).
        
         \item\textbf{Información de Puertos}: Puertos de origen y destino.
        
         \item\textbf{Características Temporales}: Variabilidad en el tiempo entre paquetes (jitter).
        
         \item\textbf{Entropía de la Carga Útil}: Medida de la aleatoriedad en los datos de la carga útil, que puede indicar cifrado o ciertos tipos de ataques.
    \end{itemize}
    
     \item\textbf{Manejo de Variables Categóricas}: Las características no numéricas, como los nombres de protocolos (TCP, UDP, ICMP), deben transformarse a un formato numérico. La técnica común de One-Hot Encoding crea nuevas columnas binarias para cada categoría, donde un '1' indica la presencia de esa categoría y un '0' su ausencia.
    
     \item\textbf{Escalado de Características Numéricas}: Las características numéricas a menudo presentan rangos de valores muy diferentes (ej., la duración de un flujo puede ser de milisegundos a horas, mientras que el número de paquetes es un entero pequeño). El escalado (normalización o estandarización) es crucial para evitar que las características con rangos más amplios dominen desproporcionadamente en el proceso de aprendizaje del modelo. Técnicas como StandardScaler (resta la media y divide por la desviación estándar) o MinMaxScaler (escala los valores a un rango específico, como [0, 1]) son comúnmente empleadas.
    
     \item\textbf{Manejo de Valores Ausentes o Ruidosos}: Los datos de red pueden contener valores faltantes o erróneos. Es fundamental aplicar estrategias de imputación (ej., rellenar con la media, mediana o moda) o eliminación de instancias para asegurar la integridad del dataset.
    
     \item\textbf{Manejo del Desequilibrio de Clases (Class Imbalance)}: En la detección de intrusiones, el tráfico legítimo es abrumadoramente más frecuente que el tráfico malicioso (clases desequilibradas). Si no se aborda, el modelo puede sesgarse hacia la clase mayoritaria y tener un rendimiento deficiente en la detección de la clase minoritaria (ataques). Técnicas como el oversampling (ej., SMOTE para crear instancias sintéticas de la clase minoritaria), undersampling (reducir la cantidad de instancias de la clase mayoritaria), o el uso de pesos de clase durante el entrenamiento del modelo, son esenciales para mitigar este problema.

\end{itemize}
\subsubsection{Evaluación de modelos en Sistemas de Detección de Intrusiones}

La evaluación rigurosa del rendimiento de un modelo de clasificación es un paso crítico en el desarrollo de un IDS basado en aprendizaje automático. Dadas las implicaciones de seguridad, es imperativo no solo medir la precisión global, sino también comprender cómo el modelo maneja los errores, especialmente los falsos negativos, que representan ataques no detectados.

La matriz de confusión es la herramienta fundamental para una evaluación detallada. Esta tabla resume las predicciones del modelo frente a las etiquetas reales del dataset y se compone de cuatro cuadrantes clave en el contexto de clasificación binaria (Normal/Ataque):

\begin{itemize}

    \item\textbf{Verdaderos Positivos (TP)}: Instancias de tráfico malicioso correctamente clasificadas como ataques.
    
    \item\textbf{Verdaderos Negativos (TN)}: Instancias de tráfico normal correctamente clasificadas como benignas.
    
    \item\textbf{Falsos Positivos (FP)}: Instancias de tráfico normal erróneamente clasificadas como ataques (falsa alarma).
    
    \item\textbf{Falsos Negativos (FN)}: Instancias de tráfico malicioso erróneamente clasificadas como normales (ataque no detectado).

\end{itemize}

A partir de la matriz de confusión, se derivan métricas esenciales para evaluar el desempeño del IDS:

\begin{itemize}

    \item\textbf{Precisión (Accuracy)}: 
        \begin{lstlisting}[language={[LaTeX]TeX},caption={Definición de una ecuación para la Precisión},label=List.Ecuacion]
        \begin{equation}
            \left(\frac{TP + TN}{TP + TN + FP + FN}\right )
            \label{Eq.Prob1}
        \end{equation}
        \end{lstlisting}

        \begin{equation}
            \left(\frac{TP + TN}{TP + TN + FP + FN}\right )
            \label{Eq.Prob2}
        \end{equation}
    
     Mide la proporción de predicciones correctas sobre el total de predicciones. Aunque intuitiva, puede ser engañosa en datasets desequilibrados, donde una alta precisión podría deberse simplemente a la correcta clasificación de la clase mayoritaria.
    
    \item\textbf{Sensibilidad / Exhaustividad (Recall / True Positive Rate - TPR)}: 
        \begin{lstlisting}[language={[LaTeX]TeX},caption={Definición de una ecuación para la Sensibilidad},label=List.Ecuacion]
        \begin{equation}
            \left(\frac{TP}{TP + TN}\right )
            \label{Eq.Prob2}
        \end{equation}
        \end{lstlisting}

        \begin{equation}
            \left(\frac{TP}{TP + TN}\right )
            \label{Eq.Prob1}
        \end{equation}

     Cuantifica la capacidad del modelo para identificar correctamente todas las instancias positivas (ataques reales). En un IDS, maximizar el recall es a menudo una prioridad, ya que un alto número de falsos negativos implica ataques que pasan desapercibidos.
    
    \item\textbf{Especificidad (True Negative Rate - TNR)}:
        \begin{lstlisting}[language={[LaTeX]TeX},caption={Definición de una ecuación para la Especificidad},label=List.Ecuacion]
        \begin{equation}
            \left(\frac{TN}{TN + FP}\right )
            \label{Eq.Prob3}
        \end{equation}
        \end{lstlisting}

        \begin{equation}
            \left(\frac{TN}{TN + FP}\right )
            \label{Eq.Prob3}
        \end{equation}
        
     Indica la proporción de instancias negativas (tráfico normal) que son correctamente identificadas.
    
    \item\textbf{Precisión (Precision / Positive Predictive Value - PPV)}: 
        \begin{lstlisting}[language={[LaTeX]TeX},caption={Definición de una ecuación para otro tipo de precisión},label=List.Ecuacion]
        \begin{equation}
            \left(\frac{TP}{TP + FP}\right )
            \label{Eq.Prob4}
        \end{equation}
        \end{lstlisting}

        \begin{equation}
            \left(\frac{TP}{TP + FP}\right )
            \label{Eq.Prob4}
        \end{equation}

     Responde a la pregunta: de todas las instancias que el modelo clasificó como ataques, ¿cuántas fueron realmente ataques? Una alta precisión minimiza los falsos positivos, lo cual es crucial para evitar la "fatiga de alerta" en los operadores de seguridad.
    
    \item\textbf{F1-Score}:
        \begin{lstlisting}[language={[LaTeX]TeX},caption={Definición de una ecuación para la f1-score},label=List.Ecuacion]
        \begin{equation}
            \left(2*\frac{Precisión*Recall}{Precisión + Recall}\right )
            \label{Eq.Prob4}
        \end{equation}
        \end{lstlisting}

         \begin{equation}
            \left(2*\frac{Precisión*Recall}{Precisión + Recall}\right )
            \label{Eq.Prob4}
        \end{equation}
    

     Es la media armónica de la precisión y la sensibilidad. Proporciona una métrica equilibrada que es especialmente útil cuando existe un desequilibrio significativo entre las clases, ya que penaliza los modelos con altos falsos positivos y falsos negativos.
    
    \item\textbf{Tasa de Falsos Positivos (False Positive Rate - FPR)}: 
        \begin{lstlisting}[language={[LaTeX]TeX},caption={Definición de una ecuación para la tasa de falsos positivos},label=List.Ecuacion]
        \begin{equation}
            \left(\frac{FP}{FP + TN}\right )
            \label{Eq.Prob5}
        \end{equation}
        \end{lstlisting}

        \begin{equation}
            \left(\frac{FP}{FP + TN}\right )
            \label{Eq.Prob5}
        \end{equation}
    

     Complementa la especificidad y es una métrica crítica en IDS, ya que una alta tasa de falsas alarmas puede llevar a que los analistas ignoren alertas legítimas.

\end{itemize}
La Curva ROC (Receiver Operating Characteristic) y el Área bajo la Curva (AUC) son herramientas gráficas y métricas complementarias. La curva ROC representa la relación entre la Tasa de Verdaderos Positivos (TPR) y la Tasa de Falsos Positivos (FPR) a distintos umbrales de clasificación, permitiendo visualizar el compromiso entre ambas. El AUC cuantifica el rendimiento general del clasificador en todos los umbrales posibles; un valor de AUC cercano a 1.0 indica un excelente poder discriminatorio.

Para garantizar la robustez y generalizabilidad del modelo, se emplean técnicas de validación. La Validación Cruzada K-Fold es un método estándar en el que el conjunto de datos se divide en k subconjuntos (folds). El modelo se entrena k veces; en cada iteración, se utiliza un fold diferente como conjunto de prueba y los k−1 restantes como conjunto de entrenamiento. Los resultados se promedian para obtener una estimación más fiable del rendimiento del modelo, reduciendo el sesgo y la varianza que podrían surgir de una única división aleatoria. Además, es esencial distinguir entre el conjunto de entrenamiento, el conjunto de validación (utilizado para el ajuste de hiperparámetros) y el conjunto de prueba (utilizado únicamente para la evaluación final imparcial del modelo).

\subsection{Ciencia de datos}\label{Sec.Referencias}

La Ciencia de Datos emerge como una disciplina transversal que integra metodologías y principios de campos como la estadística, la informática, las matemáticas, el conocimiento del dominio y la visualización, con el propósito primordial de extraer conocimiento, patrones e insights accionables a partir de volúmenes de datos complejos y heterogéneos. Su objetivo no es meramente el procesamiento de información, sino la capacidad de transformar datos brutos en inteligencia estratégica, facilitando la toma de decisiones informadas y la predicción de eventos futuros. En la actualidad, su aplicación es ubicua, abarcando desde la optimización de procesos empresariales hasta la investigación científica y, pertinentemente para este proyecto, la ciberseguridad.

Dentro del vasto espectro de la Ciencia de Datos, el aprendizaje automático (Machine Learning - ML) y el aprendizaje profundo (Deep Learning - DL) constituyen dos de sus pilares más potentes y de mayor crecimiento. Mientras que el aprendizaje automático engloba un conjunto de algoritmos que permiten a los sistemas aprender de los datos para realizar predicciones o tomar decisiones sin ser programados explícitamente para cada tarea, el aprendizaje profundo representa una subcategoría del ML que se basa en redes neuronales artificiales con múltiples capas, capaces de aprender representaciones de datos jerárquicas y abstractas.

En el marco de este proyecto de fin de grado, la Ciencia de Datos se erige como el eje metodológico para abordar el problema de la detección de intrusiones en redes. La fase inicial implica la adquisición y preprocesamiento de grandes volúmenes de datos de tráfico de red, los cuales se presentan con una multitud de características y métricas de conexión. Este proceso es crucial para transformar el tráfico en bruto en un formato estructurado y apto para el análisis computacional.

Posteriormente, y de manera exclusiva, se recurrirá a las técnicas de Machine Learning para desarrollar la capacidad predictiva del sistema. Específicamente, se ha optado por el entrenamiento de un modelo basado en el algoritmo Random Forest. Este modelo, conocido por su robustez, eficiencia y capacidad para manejar conjuntos de datos de alta dimensionalidad y características complejas, será el encargado de resolver un problema de clasificación. Su función cardinal será la de discernir, con alta fiabilidad, entre patrones de tráfico de red que corresponden a un comportamiento normal (benigno) y aquellos que denotan una actividad maliciosa o una posible intrusión. La implementación de esta aproximación permitirá que el sistema de detección evolucione y se adapte a nuevas amenazas basándose en el aprendizaje continuo de los patrones inherentes a los datos de la red.

Al escribir un documento extenso, como es el caso de una memoria de TFG, es habitual la necesidad de hacer referencia a partes del mismo (capítulo, sección, subsección) desde ciertos puntos del texto. Por ejemplo, en la Sección~\ref{Sec.Capitulos} se describe cómo dividir el documento principal en capítulos.

Crear estas referencias en \LaTeX esto es muy sencillo y se compone de dos pasos:

\begin{enumerate}
    \item Colocar una etiqueta tras la llave de cierre correspondiente al comando \verb|\chapter|, \verb|\section|, \verb|\subsection| o \verb|\subsection|, según el caso, usando para ello el comando \verb|\label{etiqueta}|.
    
    \item Desde cada punto del texto en que necesitemos hacer referencia a ese capítulo, sección o subsección usaríamos el comando \verb|\ref{etiqueta}|, que insertará el número que corresponda.
\end{enumerate}

En este propio fuente puedes ver ejemplos de uso de dichos comandos. Además de \verb|\ref|, que inserta el número de capítulo, sección, subsección o subsubsección, también se pueden usar los comandos \verb|\nameref| para obtener el título y \verb|\pageref| para obtener la página. Por ejemplo, el comando\footnote{El carácter \texttt{\~} en \LaTeX~ actúa como un espacio irrompible. Permite garantizar que las partes dispuestas a izquierda y derecha no se separarán, saltando juntas a la siguiente línea de texto en caso de que no hubiese espacio suficiente en la actual}:

\begin{verbatim}
En la Sección~\ref{Sec.Capitulos}~\nameref{Sec.Capitulos} (véase la 
pág.~\pageref{Sec.Capitulos}) se explica cómo estructurar los capítulos.
\end{verbatim}

Produce el siguiente resultado:

\textit{En la Sección~\ref{Sec.Capitulos}~\nameref{Sec.Capitulos} (véase la pág.~\pageref{Sec.Capitulos}) se explica cómo estructurar los capítulos.
}

\section{Descripción del proyecto}
El propósito principal de este proyecto es la implementación de un capturador de paquetes en tiempo real que sea capaz de capturar dichos paquetes y agruparlos en distintos flujos para que posteriormente, se muestren en una aplicación web basada en un sistema de detección de intrusiones aplicándole mecanismos de machine learning.

El proceso comienza con la captura del tráfico de red mediante una aplicación que tiene como fin la captura de paquetes en claro y a partir de estos, organiza los paquetes en distintos flujos y extrae las características necesarias para que pasen a un formato parecido al que se rige CICFlowMeter, que es una herramienta de generación y análisis de flujos de tráfico de red que produce flujos bidireccionales a partir de paquetes de red.

Por último, la aplicación mostrará en un dashboard, los flujos de paquetes en un panel, y en otro, aparecerán cuales de estos se consideran una amenaza. La lógica de control para discernir entre un flujo benigno y maligno, se encargá el modelo de IA que hemos elegido, el cual, estará integrado en el backend al igual que el propio capturador en tiempo real que hemos implementado.

El presente Proyecto de Fin de Grado aborda el diseño e implementación de un Sistema de Detección de Intrusiones (IDS) innovador y multifuncional que integra la captura de tráfico de red en tiempo real con técnicas avanzadas de aprendizaje automático (Machine Learning). El objetivo primordial es desarrollar una solución robusta capaz de discernir, con alta fiabilidad, entre tráfico de red legítimo (benigno) y actividad maliciosa o anómala.

El objetivo principal de este trabajo es la creación de una herramienta especializada para la captura y el procesamiento de paquetes de red. Esta aplicación ha sido diseñada con la capacidad de operar en tiempo real, monitorizando continuamente el flujo de datos a través de la red. Un aspecto distintivo de esta herramienta es su versatilidad para generar y persistir flujos de paquetes, transformando la información de bajo nivel de los paquetes individuales en un formato estructurado y significativo para el análisis posterior. La aplicación permite la conversión de estos flujos en formatos de salida estandarizados como .csv o .txt, lo que facilita la creación de nuevas bases de datos de tráfico de red. Esta funcionalidad es crucial, ya que emula y complementa la metodología empleada en datasets de referencia en ciberseguridad, como los de la serie CIC-IDS (ej., CIC-IDS2017, CIC-IDS2018, CIC-IDS2019), proporcionando una base para la investigación y el desarrollo futuros en el campo. Además, la capacidad de invocación por consola dota a la herramienta de flexibilidad para ser integrada en diversos entornos y automatizaciones, permitiendo la generación de datos para el entrenamiento y la evaluación de modelos de Machine Learning.

Para validar y demostrar la eficacia del capturador de paquetes y para completar la arquitectura del IDS, se ha implementado un módulo de detección basado en Machine Learning. Este módulo utiliza un modelo de clasificación Random Forest, seleccionado por su reconocida capacidad para manejar grandes volúmenes de datos con alta dimensionalidad y su robustez ante el ruido. El modelo ha sido entrenado para identificar patrones y características específicas que distinguen el tráfico normal de diversas categorías de ataques, lo que le permite clasificar de forma predictiva cada flujo de red entrante.

Complementando estas funcionalidades, se ha desarrollado una aplicación web, concebida como un dashboard intuitivo, que permite visualizar en tiempo real la actividad del capturador de paquetes y el rendimiento del modelo de detección. Este interfaz gráfico no solo ofrece una representación clara de los eventos de red, sino que también sirve como una herramienta práctica para monitorizar las detecciones de tráfico malicioso, facilitando la interacción del usuario con el IDS.

\section{Objetivos y motivación}

La proliferación incesante de las tecnologías digitales y la interdependencia sistémica de las infraestructuras de red han configurado un panorama donde la ciberseguridad ya no es una mera consideración técnica, sino un pilar fundamental para la operatividad y la resiliencia de cualquier entidad, desde organizaciones gubernamentales hasta empresas privadas y usuarios individuales. En este entorno, la capacidad de detectar y responder eficazmente a las intrusiones cibernéticas se ha vuelto crítica. Las amenazas evolucionan con una celeridad asombrosa, adoptando formas cada vez más sofisticadas y evasivas que superan las capacidades de los sistemas de defensa convencionales, como los Sistemas de Detección de Intrusiones (IDS) basados exclusivamente en firmas. Estos últimos, si bien eficientes contra amenazas conocidas y previamente catalogadas, son inherentemente limitados frente a los ataques "zero-day" o las variantes polimórficas que modifican sus patrones para eludir la detección.

Esta brecha en las capacidades de detección tradicional ha impulsado la necesidad de explorar paradigmas más adaptativos y predictivos. El aprendizaje automático (Machine Learning - ML) emerge como una solución prometedora, dada su inherente capacidad para identificar patrones complejos, anomalías y comportamientos desviados en grandes volúmenes de datos, sin requerir una programación explícita para cada posible amenaza \cite{Zhang2022AICybersecurity}. La aplicación de ML en la detección de intrusiones no solo promete una mayor tasa de detección de ataques novedosos, sino que también ofrece la posibilidad de reducir la dependencia de las actualizaciones manuales de firmas y reglas.

La motivación principal de este Proyecto de Fin de Grado radica precisamente en la respuesta a esta necesidad crítica. Se aspira a contribuir de forma tangible al campo de la ciberseguridad mediante el desarrollo de un IDS que fusione la monitorización en tiempo real del tráfico de red con la inteligencia predictiva del Machine Learning. Más allá de la mera detección, se reconoce un desafío recurrente en la investigación de IDS: la escasez de conjuntos de datos de tráfico de red etiquetados que sean representativos y actuales para el entrenamiento y la validación de modelos de Machine Learning \cite{PolaniaArias2021EvaluacionMLIDS}. La creación de tales datasets es un proceso laborioso y costoso, lo que limita el progreso en la evaluación comparativa y el desarrollo de nuevas arquitecturas de detección. Esta dificultad intrínseca proporciona una motivación adicional y un objetivo secundario crucial para este proyecto: desarrollar una herramienta que no solo detecte intrusiones, sino que también facilite la generación de datasets de tráfico de red de alta calidad, emulando las metodologías de benchmarks reconocidos como los de la serie CIC-IDS (ej., CIC-IDS2017, CIC-IDS2018, CIC-IDS2019). Esta capacidad es fundamental para impulsar futuras investigaciones y para el entrenamiento continuo de modelos más robustos y adaptativos.

Con base en esta profunda motivación y el análisis de las limitaciones existentes, los objetivos de este proyecto se estructuran de la siguiente manera:

En primer lugar, el objetivo primordial es el diseño, desarrollo e implementación de un Sistema de Detección de Intrusiones (IDS) holístico y funcional, anclado en los principios del aprendizaje automático. Este sistema debe ser capaz de analizar el tráfico de red en tiempo real, distinguiendo con precisión entre patrones de comportamiento benignos y maliciosos, para así elevar la postura de seguridad de la infraestructura monitoreada.

Para materializar este objetivo general, se han delineado varios objetivos específicos que guiarán las fases de desarrollo:

\begin{itemize}

    \item\textbf{Concebir y Construir una Herramienta Avanzada para la Captura y el Preprocesamiento de Tráfico de Red}: El desarrollo de una aplicación personalizada es fundamental. Esta herramienta no solo deberá ser eficiente en la captura de paquetes en tiempo real, sino que también integrará capacidades robustas para la reconstrucción de flujos de conexión y la extracción detallada de un amplio conjunto de características pertinentes para el análisis de seguridad. Un valor añadido de esta herramienta será su funcionalidad para generar salidas estructuradas en formatos estándar como .csv o .txt, lo que permitirá la creación sistemática de nuevas bases de datos de tráfico de red. Esta capacidad de generación de datasets, invocable incluso desde la consola, es vital para la replicabilidad, la extensibilidad y la contribución a la comunidad de investigación en ciberseguridad.
    
    \item\textbf{Investigar, Implementar y Optimizar un Modelo de Clasificación de Machine Learning de Alto Rendimiento}: La selección de un algoritmo de aprendizaje automático es crucial. Se ha optado por el algoritmo Random Forest debido a su probada eficacia en problemas de clasificación multiclase y binaria, su resistencia al sobreajuste, y su capacidad para gestionar conjuntos de datos con un elevado número de características y una significativa dimensionalidad \cite{Breiman2001RandomForests}. El modelo será sometido a un riguroso proceso de entrenamiento con datasets de tráfico etiquetado, con el fin de optimizar su rendimiento para la minimización simultánea de falsos positivos (que generan fatiga en los analistas) y, lo que es más crítico, de falsos negativos (que implican ataques no detectados) \cite{PolaniaArias2021EvaluacionMLIDS}.
    
    \item\textbf{Asegurar la Integración Fluida entre la Captura de Datos y el Motor de Detección de Machine Learning}: Se establecerá un pipeline de procesamiento de datos que permita la alimentación continua y eficiente de los flujos de red preprocesados desde la herramienta de captura hacia el modelo de Machine Learning. Esta integración garantizará que el IDS pueda clasificar el tráfico de forma dinámica y emitir alertas inmediatas ante la detección de actividades sospechosas, operando en un modo casi en tiempo real.
    
    \item\textbf{Diseñar y Desarrollar una Interfaz Web Intuitiva para la Monitorización y Visualización}: Para facilitar la interacción del usuario y la supervisión del sistema, se implementará una aplicación web que actúe como un dashboard. Esta interfaz ofrecerá una representación clara y amigable de la actividad de la red, los eventos de detección generados por el modelo de Machine Learning, y las métricas operativas del IDS. La visualización en tiempo real es clave para proporcionar a los administradores de seguridad una panorámica inmediata del estado de la red.
    
    \item\textbf{Realizar una Evaluación Exhaustiva y Rigurosa del Rendimiento del IDS Propuesto}: La validación empírica del sistema es indispensable. Se llevarán a cabo pruebas exhaustivas utilizando metodologías de evaluación estándar para problemas de clasificación en el ámbito de la ciberseguridad. Esto incluirá el cálculo y análisis de métricas como la precisión (accuracy), sensibilidad (recall), precisión (precision), F1-score, la construcción y análisis de la matriz de confusión, y la curva ROC con su respectivo AUC \cite{PolaniaArias2021EvaluacionMLIDS}. El objetivo es demostrar la eficacia del IDS en la identificación de diferentes tipos de ataques y validar su viabilidad como una herramienta de seguridad operativa y de valor añadido.
 
\end{itemize}

A través de la consecución de estos objetivos, este proyecto no solo aspira a generar una solución técnica funcional para la detección de intrusiones, sino también a contribuir al cuerpo de conocimiento en la intersección de la ciberseguridad y la inteligencia artificial, ofreciendo herramientas y metodologías que pueden ser de utilidad para la comunidad científica y profesional.

\section{Metodología de desarrollo de software}
Una metodología de desarrollo de software es un conjunto de prácticas,
técnicas y procedimientos que se utilizan para organizar, planificar y ejecutar proyectos de desarrollo de software. Su objetivo principal es mejorar la eficiencia y la calidad del proceso de desarrollo, asegurando que el software se entregue a tiempo, dentro del presupuesto y cumpla con los requisitos del cliente. Estas metodologías proporcionan un marco estructurado que guía a los equipos de desarrollo a través de las diferentes fases del ciclo de vida del software, desde la concepción y el diseño hasta la implementación, prueba y mantenimiento.

\subsection{SCRUM como modelo de desarrollo}
Dada la estructura del proyecto y sus dimensiones, se ha optado por utilizar la metodología Scrum. Scrum es una metodología ágil que se basa en la realización de incrementos pequeños y manejables, permitiendo así una mayor flexibilidad y adaptación a los cambios a lo largo del proceso de desarrollo.

Algunos de los componentes clave de Scrum son:
\begin{itemize}

    \item\textbf{Roles: Scrum define tres roles principales}:
    \begin{itemize}
 
        \item\textbf{Product Owner}: Responsable de maximizar el valor del producto y gestionar el backlog del producto.
        \item\textbf{Scrum Master}: Facilita el proceso Scrum, ayuda al equipo a seguir las prácticas de Scrum y elimina impedimentos.
        \item\textbf{Equipo de Desarrollo}: Grupo multifuncional que trabaja en la entrega de incrementos del producto.
          
    \end{itemize}
    \item\textbf{Eventos}:
    \begin{itemize}

        \item\textbf{Sprint}: Periodo de tiempo fijo (generalmente de 1 a 4 semanas) durante el cual se realiza un incremento del producto.
        \item\textbf{Sprint Planning}: Reunión para planificar el trabajo que se realizará durante el sprint.
        \item\textbf{Daily Scrum}: Reunión diaria de 15 minutos para sincronizar el trabajo del equipo.
        \item\textbf{Sprint Review}: Reunión para revisar el incremento y adaptar el backlog del producto si es necesario.
        \item\textbf{Sprint Retrospective}: Reunión para reflexionar sobre el sprint y buscar mejoras continuas.
           
    \end{itemize}
    \item\textbf{Artefactos}:
    \begin{itemize}

        \item\textbf{Product Backlog}: Lista priorizada de todo el trabajo que se necesita en el producto.
        \item\textbf{Sprint Backlog}: Lista de tareas seleccionadas del product backlog para completarse en el sprint actual.
        \item\textbf{Increment}: El resultado de un sprint, que debe ser un producto utilizable y potencialmente desplegable.
          
    \end{itemize}
   
\end{itemize}

Scrum se caracteriza por su enfoque en la transparencia, inspección y
adaptación, permitiendo a los equipos responder rápidamente a los cambios y mejorar continuamente.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.8\textwidth]{imagenes/proceso_scrum.png}
  \caption{Proceso de las fases de desarrollo SCRUM.}
  \label{Fig.FasesSCRUM}
\end{figure}

\subsection{Adaptación de SCRUM para proyectos individuales}
Aunque Scrum está diseñado originalmente para equipos, sus principios y
estructura pueden adaptarse para el desarrollo por una sola persona. Basándonos en el artículo “Scrum for One” de Lucidchart [5] vamos a desarrollar el proceso de adaptación de esta metodología para una sola persona. 

En un entorno de un solo desarrollador, la misma persona asume los roles de Product Owner, Scrum Master y Equipo de Desarrollo. Esto requiere una clara organización y autogestión. El desarrollador único debe gestionar el backlog del producto, facilitar su propio proceso de desarrollo y eliminar impedimentos de manera autónoma. Este enfoque puede ser beneficioso ya que el desarrollador tiene control total sobre las decisiones y priorizaciones, lo que puede agilizar el proceso. 

Se pueden mantener los eventos clave de Scrum, pero de manera simplificada y flexible para una sola persona:
\begin{itemize}
    
    \item\textbf{Sprint Planning}: Planificar los sprints sigue siendo esencial. El desarrollador único debe dedicar tiempo a establecer objetivos claros para cada sprint, definir las tareas necesarias y priorizarlas en el sprint backlog.
    
    \item\textbf{Daily Scrum}: Aunque no hay un equipo con el cual sincronizarse, el desarrollador puede realizar una auto-revisión diaria. Esto ayuda a mantener el enfoque y la disciplina, permitiendo reflexionar sobre el progreso y ajustar el plan si es necesario.
    
    \item\textbf{Sprint Review}: Al final de cada sprint, el desarrollador revisa el trabajo completado. Este evento puede incluir la evaluación de cómo se han alcanzado los objetivos del sprint y la identificación de cualquier ajuste necesario en el backlog del producto.

    \item\textbf{Sprint Retrospective}: Es crucial para la mejora continua. El desarrollador reflexiona sobre lo que funcionó bien y lo que se puede mejorar para los próximos sprints.
  
\end{itemize}

Los artefactos de Scrum también siguen siendo importantes ya que una vez
vistos las modificaciones en los eventos se puede concluir que los artefactos se usarán de la misma forma que en un equipo con varias personas. 

Además, la naturaleza iterativa de Scrum permite una gran flexibilidad y
capacidad de adaptación. Esto significa que el desarrollador puede responder rápidamente a los cambios y ajustar su enfoque según sea necesario, lo cual es vital en un entorno de desarrollo dinámico. Las retrospectivas regulares fomentan una cultura de mejora continua, permitiendo identificar áreas de mejora y aplicar cambios en ciclos cortos, lo que facilita la evolución y optimización del proceso de desarrollo.

\subsection{Sprints de desarrollo}\label{Sec.Sprints}
El desarrollo de un sistema tan complejo como un Sistema de Detección de Intrusiones (IDS) que integra captura de tráfico en tiempo real, procesamiento de datos, análisis mediante aprendizaje automático y una interfaz web, exige una metodología ágil que permita gestionar la complejidad, adaptarse a los desafíos emergentes y entregar valor de forma incremental. Para este proyecto, se adoptó un enfoque basado en Sprints de Desarrollo, inspirados en la metodología SCRUM. Esta aproximación facilitó la división del proyecto en iteraciones cortas y manejables, cada una enfocada en la consecución de funcionalidades específicas, garantizando así un progreso continuo y la capacidad de integrar retroalimentación a lo largo del ciclo de vida del desarrollo. A continuación, se describen los sprints planificados y ejecutados, delineando los objetivos y entregables clave de cada fase.


\textbf{Sprint 1: Fundamentos Teóricos y Análisis de Requisitos}

Este sprint inaugural se centró en la construcción de una base de conocimiento sólida indispensable para abordar un proyecto de la envergadura de un IDS basado en Machine Learning. Las primeras semanas se dedicaron a una inmersión profunda en la literatura científica y técnica. Se revisó una colección de papers y artículos especializados proporcionados por la dirección del proyecto, enfocados en la intersección de la inteligencia artificial y la ciberseguridad, con especial énfasis en los principios y arquitecturas de los Sistemas de Detección de Intrusiones \cite{Charmet2022XAI}, \cite{Zhang2022AICybersecurity}. Paralelamente, se llevó a cabo una investigación exhaustiva en bases de datos académicas y repositorios en línea para complementar el entendimiento de conceptos fundamentales de IDS y los distintos paradigmas de inteligencia artificial, inicialmente explorando tanto el aprendizaje supervisado como el no supervisado. Una vez afianzado el conocimiento sobre las técnicas de aprendizaje automático, la atención se dirigió hacia la identificación y análisis de conjuntos de datos (datasets) adecuados para el entrenamiento y evaluación de modelos de IDS. Se familiarizó con la estructura, características y taxonomía de datasets de referencia en la comunidad, tales como CIC-IDS (2017, 2018, 2019), KDD99, NSL-KDD, y UNSW \cite{PolaniaArias2021EvaluacionMLIDS}, comprendiendo sus particularidades y la información que estos proporcionan para la detección de anomalías. Este sprint concluyó con una clara comprensión del marco teórico y una dirección definida hacia la aplicación de técnicas de aprendizaje supervisado, sentando las bases conceptuales para las fases de implementación.

\textbf{Sprint 2: Diseño del Módulo de Captura y Selección de Tecnologías} 

Con la base teórica establecida, el segundo sprint se focalizó en el diseño arquitectónico del módulo de captura de paquetes y la selección de las tecnologías más adecuadas. La fase inicial de diseño incluyó la evaluación de posibles integraciones con herramientas existentes como Wireshark; sin embargo, las complejidades asociadas a su código base y las potenciales dificultades de integración con componentes de IA llevaron a su descarte. Se llevó a cabo una investigación profunda sobre los lenguajes de programación y las librerías disponibles para el desarrollo de capturadores de paquetes eficientes. Esta fase concluyó con la elección de Python, un lenguaje que demostró ser idóneo por su robustez, su amplio ecosistema de librerías para la manipulación de redes \cite{Shrefler2017Networking} y su facilidad de integración con frameworks de Machine Learning \cite{Pedregosa2011ScikitLearn}. Crucialmente, se identificó una API especializada que proporcionaba un conjunto rico de funcionalidades para la extracción de características detalladas a partir del tráfico de red. Al finalizar este sprint, se dispuso de un diseño conceptual claro del capturador y una pila tecnológica definida para su implementación.

\textbf{Sprint 3: Implementación Básica del Capturador de Paquetes y CLI}

Este sprint se dedicó a la implementación inicial del capturador de paquetes en Python. Se desarrolló la funcionalidad core que permite la intercepción y procesamiento de paquetes de red. El foco principal fue la creación de una interfaz de línea de comandos (CLI) que posibilitara la invocación del programa con parámetros específicos, tales como la interfaz de red a monitorear. Se logró que el capturador pudiera iniciar y detener la recolección de flujos de paquetes desde la terminal, sentando las bases operativas de la herramienta. Las pruebas iniciales se centraron en verificar la correcta captura y el parseo básico de los datos de los paquetes, asegurando que el flujo de información se manejara de manera estable.

\textbf{Sprint 4: Desarrollo de la Funcionalidad de Exportación de Datos}

Continuando con el módulo de captura, el cuarto sprint se concentró en una funcionalidad crítica para la utilidad del proyecto: la exportación de los flujos de paquetes procesados a formatos estructurados. Reconociendo la dificultad de obtener datasets de IDS actualizados, se implementó la capacidad de generar archivos en formato .csv y .txt a partir de los flujos capturados. Esta funcionalidad es fundamental, ya que permite que el IDS contribuya a la creación de nuevas bases de datos de tráfico de red, emulando la estructura de datasets de referencia como los de la serie CIC-IDS. Esta capacidad es clave para la investigación y el desarrollo de modelos de Machine Learning, al proporcionar acceso a datos más representativos del tráfico moderno. Se realizaron pruebas exhaustivas para validar la integridad y el formato de los archivos generados, asegurando que pudieran ser utilizados directamente para el preprocesamiento y entrenamiento de modelos de IA.

\textbf{Sprint 5: Definición de la Arquitectura Web y Prototipado de la Interfaz}

Con el módulo de captura en progreso, este sprint abordó el diseño y la arquitectura de la aplicación web que albergaría el IDS completo. Se llevó a cabo una investigación de frameworks de desarrollo web full-stack, evaluando opciones como Angular, React (para frontend) combinados con backends como Laravel (PHP) o Flask/Django (Python). La decisión final se inclinó por Reflex, un framework de Python que ofrecía una solución unificada para el desarrollo full-stack en un solo lenguaje, lo que simplificaba la integración con los componentes de Python existentes y optimizaba el flujo de trabajo. Una vez seleccionado el framework, se procedió al análisis detallado y el diseño de la interfaz de usuario (UI). Esto incluyó la elaboración de diagramas de flujo de usuario, la creación de mockups visuales y wireframes para definir la estructura y navegación del dashboard. El objetivo fue crear un diseño intuitivo que facilitara la visualización del tráfico de red y las futuras detecciones.

\textbf{Sprint 6: Implementación del Frontend y Conexión al Backend del Capturador}

Este sprint se centró en la construcción del frontend de la aplicación web según los diseños establecidos en el sprint anterior. Se implementaron las principales pantallas y componentes de la interfaz de usuario, dando vida al dashboard. Simultáneamente, se inició el desarrollo de la capa de backend de la aplicación web, preparada para recibir y procesar los datos del capturador. Un hito crítico de este sprint fue la integración inicial del módulo de captura de paquetes con el backend de la aplicación web. Para lograr una comunicación en tiempo real y asíncrona entre ambos componentes, se implementó un sistema basado en colas síncronas. Las pruebas de este sprint se dirigieron a verificar que los datos de tráfico capturados por el módulo de Python se transmitieran correctamente al backend y que el panel de visualización del tráfico de red en el frontend se actualizara en tiempo real, confirmando la operatividad del flujo de datos de extremo a extremo.

\textbf{Sprint 7: Preprocesamiento y Unificación del Dataset para ML}

Paralelo al avance de la aplicación web, este sprint se dedicó intensivamente a la preparación del conjunto de datos para el entrenamiento del modelo de Machine Learning. Se seleccionó el CIC-IDS2018 como dataset principal debido a su representatividad y variedad de ataques \cite{PolaniaArias2021EvaluacionMLIDS}. La tarea crucial fue la unificación de los diversos ficheros que componían este dataset (correspondientes a diferentes días de captura y tipos de ataque) en una única base de datos cohesiva. Esta consolidación fue fundamental para asegurar que el modelo de aprendizaje tuviera acceso a un volumen variado y robusto de flujos de tráfico, crucial para su capacidad de generalización \cite{PolaniaArias2021EvaluacionMLIDS}. Tras la unificación, se llevó a cabo un exhaustivo proceso de preprocesamiento, análisis y limpieza de los datos. Esto incluyó la gestión de valores ausentes, la corrección de inconsistencias, la normalización/estandarización de características numéricas y la codificación de variables categóricas, preparando el dataset para la fase de entrenamiento \cite{James2013ISLR}.

\textbf{Sprint 8: Entrenamiento y Evaluación del Modelo de Machine Learning (Random Forest)}

Una vez que el dataset estuvo preparado y limpio, este sprint se centró por completo en el desarrollo y la evaluación del modelo de Machine Learning. Se procedió al entrenamiento del clasificador Random Forest, un algoritmo elegido por su eficacia en la detección de intrusiones en entornos de alta dimensionalidad y su robustez ante el sobreajuste \cite{Breiman2001RandomForests}. Durante esta fase, se ajustaron los hiperparámetros del modelo para optimizar su rendimiento. Tras el entrenamiento, se realizó una evaluación rigurosa del modelo utilizando métricas clave de clasificación como la precisión (accuracy), la sensibilidad (recall), la precisión (precision), el F1-score, y el análisis detallado de la matriz de confusión y la curva ROC con su Área bajo la Curva (AUC) \cite{PolaniaArias2021EvaluacionMLIDS}. Este análisis permitió determinar la capacidad predictiva del modelo y su fiabilidad para distinguir entre tráfico benigno y malicioso. El entregable de este sprint fue un modelo de Machine Learning entrenado y validado, listo para ser integrado en el sistema en tiempo real.

\textbf{Sprint 9: Integración Final del Modelo ML y Detección en Tiempo Real}

Este sprint fue el punto culminante de la integración de todos los componentes del IDS. El modelo de Machine Learning entrenado fue integrado en el backend de la aplicación web. Esto permitió que los flujos de paquetes, capturados por el módulo Python y transmitidos a través de las colas síncronas al backend, fueran ahora analizados en tiempo real por el modelo Random Forest. La funcionalidad clave de este sprint fue la capacidad del sistema para determinar si un flujo de tráfico era benigno o malicioso de forma automática y mostrar esta clasificación instantáneamente en la interfaz web. Se realizaron pruebas de integración de extremo a extremo para asegurar que el ciclo completo, desde la captura hasta la predicción y la visualización, operara sin fallos y con la menor latencia posible.

\textbf{Sprint 10: Pruebas Exhaustivas, Optimización y Validación Final}

El sprint final se dedicó a la consolidación y refinamiento de todo el sistema. Se ejecutaron multitud de pruebas exhaustivas y multifacéticas para garantizar la robustez, la estabilidad y el rendimiento óptimo de la aplicación en su conjunto. Esto incluyó pruebas de estrés para verificar la capacidad del capturador y del modelo bajo cargas de tráfico elevadas, pruebas de usabilidad de la interfaz web, y pruebas de regresión para asegurar que las nuevas funcionalidades no introdujeran fallos en componentes existentes. Asimismo, se llevó a cabo una fase de optimización general del rendimiento de la aplicación, buscando mejorar los tiempos de respuesta del capturador, la latencia en la predicción del modelo y la fluidez de la interfaz de usuario. Finalmente, se realizó una validación completa del sistema \cite{NIST2020SP800-115} para confirmar que todos los objetivos del proyecto habían sido alcanzados, que el IDS operaba de manera confiable y que estaba listo para su defensa.

\section{Prespuesto del proyecto}

La planificación y ejecución de cualquier proyecto, incluyendo un Trabajo Fin de Grado, conlleva una serie de costes asociados que deben ser estimados y desglosados para comprender la inversión económica necesaria. Este apartado detalla el presupuesto aproximado del desarrollo del Sistema de Detección de Intrusiones (IDS) y la herramienta de generación de datasets a lo largo de los ocho meses de duración estimados para el proyecto. Para la elaboración de este presupuesto, se han considerado los recursos humanos implicados, los recursos materiales utilizados y los costes indirectos asociados.

\textbf{Recursos Humanos}

Aunque este proyecto ha sido realizado por un único estudiante en el marco de su Trabajo Fin de Grado, para fines de presupuesto y para reflejar un escenario de desarrollo profesional, se estimará el coste equivalente a la contratación de un Programador Junior / Investigador Junior en Ciberseguridad. Este perfil asumiría las responsabilidades de diseño, programación, investigación y gestión del proyecto.

Para la estimación salarial, se puede tomar como referencia el Convenio colectivo estatal de empresas de consultoría y estudios de mercado y de la opinión pública, o bien, promedios de salarios para perfiles junior en el sector tecnológico en España. Considerando un salario anual bruto para un programador/investigador junior en un rango de 18.000€ a 22.000€ (más realista para el perfil y las habilidades requeridas en Machine Learning y ciberseguridad que los 14.800,66€ mencionados en un convenio específico que puede no aplicar directamente al rol técnico avanzado), tomaremos un valor medio de 20.000€ anuales para este cálculo.
\begin{itemize}

    \item\textbf{Salario base anual estimado}: 20.000,00 \euro{}
    
    \item \textbf{Salario mensual:}$\displaystyle \frac{20.000,00}{12} = 1.666,67 $ \euro{}/mes
   
    \item\textbf{Coste total de recursos humanos (8 meses)}:$\displaystyle 1.666,67 \text{ \euro{}/mes} \times 8 \text{ meses} = 13.333,36 $ \euro{}

\end{itemize}

A este coste salarial se le deberían añadir los costes sociales (aportaciones a la Seguridad Social por parte de la empresa), que en España suelen rondar el 30-35\% del salario bruto. Para una estimación, consideraremos un 32\%.
\begin{itemize}
    
    \item\textbf{Costes Sociales mensuales}:$\displaystyle 1.666,67 \text{ \euro{}/mes} \times 0.32 = 533,33 $ \euro{}/mes
    
    \item\textbf{Costes Sociales totales (8 meses)}:$\displaystyle 533,33 \text{ \euro{}/mes} \times 8 \text{ meses} = 4.266,64 $ \euro{}
    
    \item\textbf{Coste total de Recursos Humanos (Salario + Costes Sociales)}:$\displaystyle 13.333,36 \text{ \euro{}} + 4.266,64 \text{ \euro{}} = 17.600,00 $ \euro{}
    
\end{itemize}

\textbf{Recursos Materiales}
 
El desarrollo del proyecto ha requerido el uso de equipamiento informático para la programación, la ejecución de modelos de Machine Learning y la realización de pruebas de captura de tráfico en tiempo real. Aunque los equipos ya eran propiedad del desarrollador, para el presupuesto se considerará una amortización proporcional al tiempo de uso del proyecto.

Se han utilizado los siguientes equipos principales:

\begin{itemize}

    \item\textbf{Ordenador Portátil (ej. HP Pavilion 16-a0003N o similar)}:
        \begin{itemize}

            \item\textbf{Coste de adquisición}: 899,57 \euro{} 
            
            \item\textbf{Vida útil estimada}: 4 años (48 meses)
            
            \item\textbf{Amortización mensua}l:$\displaystyle \frac{899,57}{48} = 18,74 $ \euro{}/mes
            
            \item\textbf{Coste por 8 meses}:$\displaystyle 18,74 \text{ \euro{}/mes} \times 8 \text{ meses} = 149,92 $ \euro{}
                   
        \end{itemize}
    
    \item\textbf{Smartphone (ej. Xiaomi Mi 10 5G o similar, para pruebas de red móvil si aplicase, o como dispositivo de test para la app web)}:
    
        \begin{itemize}
        
            \item\textbf{Coste de adquisición}: 300,00 \euro{} (basado en la referencia del compañero)
            
            \item\textbf{Vida útil estimada}: 3 años (36 meses)
            
            \item\textbf{Amortización mensual}:$\displaystyle \frac{300,00}{36} = 8,33 $ \euro{}/mes
            
            \item\textbf{Coste por 8 meses}:$\displaystyle 8,33 \text{ \euro{}/mes} \times 8 \text{ meses} = 66,64 $ \euro{}
                
        \end{itemize}
    
    \item\textbf{Software y Herramientas (Licencias/Suscripciones)}:
    Aunque la mayoría de las herramientas utilizadas (Python, librerías como Scikit-learn, Reflex, Wireshark/Scapy, etc.) son de código abierto o gratuitas, en un entorno profesional se considerarían costes para herramientas de desarrollo (IDEs avanzados), sistemas de control de versiones premium (GitHub Enterprise), o servicios de computación en la nube para el entrenamiento de modelos más grandes (ej. Google Colab Pro, AWS, Azure, etc.). Para este TFG, asumiremos costes mínimos o nulos por licencias de software, pero se podría estimar un coste simbólico por el acceso a ciertos recursos o licencias de herramientas de diseño/gestión si se hubieran utilizado versiones de pago. Aquí asumiremos 0€ dado el contexto de TFG, pero se podría incluir un pequeño monto (ej. 50-100€) para una licencia de IDE o software de diseño.
    
    \item\textbf{Conexión a Internet y Electricidad}:
    Estos costes se suelen incluir en los costes operativos indirectos de un proyecto. Considerando una parte proporcional del coste de una conexión doméstica y el consumo eléctrico del equipo durante las horas de desarrollo.
    
        \begin{itemize}
        
            \item\textbf{Conexión a Internet}:$\displaystyle \frac{50 \text{ \euro{}/mes}}{2} \text{ (uso profesional)} \times 8 \text{ meses} = 200,00 $ \euro{}
            
            \item\textbf{Electricidad}:$\displaystyle \frac{30 \text{ \euro{}/mes}}{2} \text{ (uso profesional)} \times 8 \text{ meses} = 120,00 $ \euro{}
                    
        \end{itemize}    
    
    \item\textbf{Total de Recursos Materiales}:$\displaystyle 149,92 \text{ \euro{}} + 66,64 \text{ \euro{}} + 0,00 \text{ \euro{}} + 200,00 \text{ \euro{}} + 120,00 \text{ \euro{}} = 536,56 $ \euro{}
   
\end{itemize}

\textbf{Costes Indirectos y Contingencias} 

Estos costes cubren gastos generales y posibles imprevistos no directamente asignables a las categorías anteriores. Incluyen, por ejemplo, material de oficina, formación específica no prevista, gastos de comunicación, etc. Se suele estimar un porcentaje sobre el total de los costes directos.
\begin{itemize}
    \item\textbf{Costes Directos Totales (Recursos Humanos + Recursos Materiales)}: $\displaystyle 17.600,00 \text{ \euro{}} + 536,56 \text{ \euro{}} = 18.136,56 \text{ \euro{}}$
    
    \item\textbf{Contingencias (10\% de los costes directos)}:$\displaystyle 18.136,56 \text{ \euro{}} \times 0.10 = 1.813,66 \text{ \euro{}}$
\end{itemize}

\textbf{Resumen del presupuesto final del proyecto}

En la siguiente tabla ~\ref{Tabla.Presupuesto} se puede observar el resumen del desglose de los costes estimados para el desarrollo de este Trabajo Fin de Grado durante un periodo de 8 meses:

\input{tablas/tablaPresupuesto}

Este presupuesto final de aproximadamente 19.910,22 euros proporciona una estimación realista del valor de mercado de un proyecto de esta envergadura y complejidad si se llevara a cabo en un entorno profesional, abarcando la investigación, el desarrollo de un módulo de captura especializado, la implementación de un modelo de Machine Learning y la creación de una aplicación web interactiva.

\section{Planificación de tiempo y costes}
La gestión del tiempo es un pilar fundamental en la ejecución exitosa de cualquier proyecto, especialmente en un Trabajo Fin de Grado (TFG) que requiere una secuencia lógica de actividades y una dedicación constante. La planificación temporal de este proyecto se ha basado en la metodología ágil de \textbf{Sprints de Desarrollo} citados en el apartado ~\ref{Sec.Sprints}, la cual ha permitido una organización estructurada de las tareas, la adaptación a los desafíos y la entrega incremental de funcionalidades, tal como se detalló en la sección de Metodología.

Para la estimación y seguimiento del tiempo, se ha considerado un período de ocho meses, que abarca desde la fase inicial de los fudamentos teóricos hasta la validación y documentación final. La duración de cada sprint fue estimada en dos semanas aproximadamente, permitiendo flexibilidad para la investigación y el desarrollo intensivo de cada módulo. Este enfoque se alinea con prácticas comunes en la planificación de proyectos de ingeniería de software, buscando optimizar el esfuerzo y los recursos disponibles [9].

Para este proyecto, el cronograma se presenta como una secuencia de los sprints ya definidos, cada uno con sus objetivos específicos y una duración estimada.

\subsection{Cronograma del Proyecto}

A continuación, se presenta el cronograma detallado del proyecto, desglosado por los sprints de desarrollo en la figura ~\ref{Fig.Cronograma}. Este cronograma visualiza la secuencia de las actividades principales y la estimación de su duración en meses.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\textwidth]{imagenes/cronograma.png}
  \caption{Diagrama de Gantt del proyecto. Fuente: Elaboración propia.}
  \label{Fig.Cronograma}
\end{figure}


Este cronograma proporciona una guía para la ejecución del proyecto, destacando la superposición de algunos sprints (como la investigación teórica con el diseño inicial, o el preprocesamiento de datos con el desarrollo web) para optimizar el tiempo. La naturaleza iterativa de la metodología ágil permite ajustes en la duración de los sprints según las necesidades emergentes y la complejidad de las tareas [7]. La dedicación principal al proyecto se concentró en la ejecución de estos sprints, culminando con una fase intensiva de integración y pruebas antes de la entrega final.

\subsection{Estimación del Esfuerzo}

La estimación del esfuerzo para el presente Trabajo de Fin de Grado se ha realizado en conformidad con la carga académica establecida para un TFG de 12 créditos ECTS, lo que se traduce en un total de 300 horas de trabajo dedicado. Este volumen de esfuerzo se ha distribuido estratégicamente a lo largo de la duración del proyecto, que abarca desde Noviembre de 2024 hasta finales de Julio de 2025, como se detalla en el cronograma.

Para un perfil de Programador/Investigador Junior, la dedicación requerida para un proyecto de esta envergadura se traduce en una asignación meticulosa de horas a cada fase, cubriendo desde la investigación y el diseño hasta la implementación, depuración y la elaboración de la documentación. Este enfoque es consistente con las prácticas comunes en la planificación de proyectos de software.

El esfuerzo total de 300 horas se ha distribuido entre las distintas fases y actividades del proyecto, alineándose con la metodología de sprints empleada. A continuación, se detalla la asignación de estas horas en las principales etapas:

\begin{itemize}

    \item\textbf{Análisis y Diseño}: Incluye la investigación teórica, la definición de requisitos funcionales y no funcionales, y la conceptualización de la arquitectura inicial del sistema.
    
    \item\textbf{Desarrollo e Implementación}: Abarca la codificación y construcción de los diversos módulos, como el capturador de tráfico, la herramienta de generación de datasets, el componente de Machine Learning y la interfaz de usuario web.
    
    \item\textbf{Pruebas y Validación}: Comprende la ejecución de baterías de pruebas, la validación del rendimiento del modelo de Machine Learning y la verificación integral de la funcionalidad del sistema.
    
    \item\textbf{Documentación}: Incluye la redacción exhaustiva de esta memoria final de grado, la cual culminará a finales de julio de 2025, y la preparación de otros materiales de apoyo.
    
    Esta distribución del esfuerzo, similar a la aplicada en otros proyectos académicos que buscan cumplir con un número de créditos específicos, asegura una asignación proporcional de recursos al trabajo técnico, la investigación y la formalización documental. La planificación detallada y el progreso de estas actividades se visualizan en el Diagrama de Gantt del proyecto, presentado en la Figura ~\ref{Fig.Cronograma}.


\end{itemize}
\section{Estructura de la memoria}
La memoria final de este proyecto está organizada en una serie de capítulos que se ajustan a la siguiente estructura:

\begin{itemize}
    \item{Capítulo 2}: Capítulo en el que se discute el estado del arte de sistema de ciberseguridad que apican ML, como diversas líneas de investigación y desarrollo al respecto
    \item{Capítulo 3}: Capítulo en el que se abordan los materiales y métodos empleados en el desarrollo del proyecto, haciéndose especial énfasis a cada una de las funcionalidades desarrolladas tanto en el capturador de paquetes como en el propio modelo de machine learning empleado para la construcción de la aplicación final para usuarios.
    \item{Capítulo 4}: Capítulo en el que se profundiza en el análisis del sistema propuesto. Se especifican los requisitos funcionales y no funcionales que definen las funcionalidades y limitaciones de la aplicación. Además, se presentan diagramas de casos de uso, de secuencia y de flujo, que ilustran a un alto nivel la arquitectura y el comportamiento del sistema.
    \item{Capítulo 5}: Este capítulo documenta el proceso de diseño detallado del sistema, traduciendo los requisitos en una arquitectura concreta. Se presentan los modelos de datos, los diagramas de clases y la estructura modular de la aplicación, definiendo cómo interactúan los componentes del capturador, el modelo de Machine Learning y la interfaz de usuario para construir una solución robusta y escalable.
    \item{Capítulo 6}: En este capítulo se describen los resultados obtenidos tras la implementación y las pruebas del sistema. Se detallan las métricas de rendimiento del modelo de Machine Learning y los hallazgos más relevantes de la evaluación. Se presentan los resultados de la validación del sistema para verificar que cumple con los requisitos iniciales.
    \item{Capítulo 7}: Este apartado presenta las conclusiones del proyecto. Se resumen los logros alcanzados, los desafíos superados y las principales lecciones aprendidas durante el desarrollo. También se discuten las limitaciones del sistema y se proponen posibles mejoras, extensiones y futuras líneas de investigación.
    \item{Capítulo 8}: Este capítulo incluye cualquier material adicional que, aunque no es esencial para la comprensión del texto principal, complementa la información de la memoria. Incluye detalles técnicos adicionales sobre la instalación y configuración del sistema, capturas de pantalla de la interfaz y manual de usuario.
    \item{Capítulo 9}: Este capítulo contiene un glosario de términos clave, acrónimos y abreviaturas utilizados en la memoria. Su propósito es ayudar al lector a comprender la terminología técnica del proyecto, garantizando la claridad y la consistencia a lo largo del documento.
    
\end{itemize}
\section{Texto en negrita, cursiva y monoespacio}

Los estilos tipográficos de las distintas partes de la memoria, como son los títulos de capítulos, secciones o los propios párrafos de texto, están predeterminados. En el contenido podemos emplear esencialmente tres variantes:

\begin{itemize}
    \item \textbf{Negrita:} debe usarse de forma puntual para \textbf{destacar algo importante}, evitando abusar de ello ya que si hay mucho texto en negrita el efecto tendrá poca utilidad. Se usa el comando \verb|\textbf{texto en negrita}|.
    
    \item \textit{Cursiva:} se emplea generalmente para introducir términos en otro idioma, por ejemplo GPU (\textit{Graphics Processing Unit}), y esporádicamente con otros fines. Se usa el comando \verb|\textit{texto en cursiva}|.
    
    \item \texttt{Monoespacio}: se usa para diferenciar nombres de variables, funciones, archivos y, en general, todo lo relacionado con código, por ejemplo: la función \texttt{printf()} de C. Se usa el comando \verb|\texttt{texto en monoespacio}|.
\end{itemize}

Aunque \LaTeX~ nos permite trabajar con colores (\verb|\textcolor{color}{texto}|), diferentes tamaños de fuente (\verb|\normalsize|, \verb|\small|, \verb|\footnotesize|, etc.), recuadros con fondo de color, etc., en la normativa de TFG de la EPSJ no se especifica que pueda hacerse uso de los mismos, por lo que se desaconseja hacerlo.

\section{Cómo insertar notas}

Las notas a pie de página\footnote{Estas notas se numeran automáticamente dentro de cada capítulo de la memoria.} son un recurso útil para aclarar algo o facilitar información complementaria sin romper el hilo de lo que está tratándose en el texto. Al igual que el texto en negrita, es un recurso del que tampoco debe abusarse, empleándose únicamente cuando es estrictamente necesario.

Para incluir una nota al pie nos serviremos del comando \verb|\footnote{Texto}|, colocándolo allí donde queremos que aparezca el número de la nota como superíndice, tal y como se aprecia en el fuente \LaTeX~ de este mismo texto.

En casos excepcionales, cuando ya no queda espacio disponible en la página actual, una nota al pie puede desplazarse a la página siguiente de donde se hace referencia a ella.

\section{Cómo insertar listas numeradas y sin numerar}

Un recurso muy habitual al redactar una memoria son las listas, tanto numeradas como sin numerar. Por su disposición en el texto, con el sangrado y las marcas que preceden cada elemento, es recomendable usar listas para cualquier enumeración de elementos o descripción de procedimientos compuestos de múltiples pasos. La lisa numerada se debería usar siempre y cuando el orden sea importante, en caso contrario es preferible una lista sin numerar.

Las listas, al igual que otros elementos en \LaTeX, puede ocupar múltiples párrafos de texto, incluso extendiéndose por varias páginas, de ahí la necesidad de marcar su inicio y su fin, definiendo lo que se denomina \textbf{un ámbito} o \textit{environment}. Los ámbitos siempre se inician con el comando \verb|\begin{}| y se finalizan con el comando \verb|\end{}|. El tipo de ámbito, y por tanto su contenido, viene determinado por el nombre que dispongamos entre las llaves.

Para el caso de las listas usaremos dos ámbitos distintos:
    \begin{itemize}
        \item \verb|itemize|: para generar una lista no numerada
        \item \verb|enumerate|: para producir una lista numerada
    \end{itemize}

En el interior del ámbito usaremos el comando \verb|\item| para ir introduciendo los elementos que forman la lista, por ejemplo:

\begin{lstlisting}[language={[LaTeX]TeX},caption={Creación de una lista no numerada con dos elementos},label=List.Itemize]
    \begin{itemize}
        \item \texttt{itemize}: para generar una lista no numerada
        \item \texttt{enumerate}: para producir una lista numerada
    \end{itemize}
\end{lstlisting}

Las listas pueden anidarse, es decir, podemos tener una lista dentro de otra, independientemente de cuál sea el tipo de cada una. El sangrado, tipo de boliche\footnote{La marca que aparece al inicio de cada elemento de una lista no numerada} y esquema de numeración se ajustarán de forma automática.

\section{Cómo insertar figuras}

A lo largo de una memoria de TFG es preciso incluir distintos tipos de figuras: diagramas que describen cómo funciona un sistema, ilustraciones y gráficas que representan o evalúan unos resultados, capturas de pantalla de un programa, etc. 

\subsection{Aspectos importantes al usar figuras}

Las normas a tener presentes a la hora de introducir figuras en la memoria son las indicadas a continuación:

\begin{itemize}
    \item Todas las figuras habrán de estar numeradas, usando una numeración consecutiva para toda la memoria o bien relativa a cada capítulo.
    
    \item Todas las figuras han de contar con un pie de figura que describa claramente qué representa. No ocurre nada si el pie de figura es extenso y ocupa varias líneas.
    
    \item Todas las figuras han de estar referenciadas desde el texto de la memoria, es decir, en algún punto de los párrafos previos o posteriores ha de aparece una referencia del tipo «\textit{como se aprecia en la Figura~N}».
    
    \item \textbf{Importante:} todas las figuras deberán indicar su procedencia, de no indicarse se entenderá que son de elaboración propia.
\end{itemize}

Trata siempre de que las figuras tengan una calidad suficiente, que en el documento resultante no se vean borrosas o pixeladas, ya que causan un mal efecto.

\subsection{Cómo almacenar las figuras}

En ocasiones las figuras a usar serán capturas de pantalla, en otras gráficas producidas con algún programa y otras imágenes procedentes de alguna fuente. Siempre que sea posible, por ejemplo para gráficas propias generadas a partir de datos o diagramas confeccionados con alguna aplicación, se recomienda almacenar la imagen en un formato vectorial, como puede ser \texttt{EPS} o \texttt{PDF}, ya que al insertarse en el documento generalmente conservan una mejor calidad.

Para imágenes no vectoriales puedes usar los formatos \texttt{PNG} y \texttt{JPEG}. Dependiendo de la resolución de la pantalla en que se haga la captura, la imagen tendrán mayor o menor calidad. En caso necesario, se puede usar la utilidad \texttt{convert} del software ImageMagick (\url{legacy.imagemagick.org}) para incrementar el número de puntos por pulgada, de manera que al insertar la imagen en el PDF mejore su calidad.

Se recomienda dar una denominación adecuada a los archivos donde se guardan las figuras, de forma que sea fácil de recordar para luego introducirlas en el fuente \LaTeX. Asimismo, es recomendable llevar las imágenes a una carpeta aparte, ya sea general para todas las figuras o bien por capítulo.

\subsection{Cómo insertar las figuras en el documento}

El comando para incluir un archivo de tipo gráfico, ya sea vectorial o no, en un documento es \verb|\includegraphics[]{}|. Entre los corchetes se usarán habitualmente los parámetros \texttt{width} o \texttt{height} para establecer el ancho o alto que queremos dar a la figura. Lo más recomendable es usar dimensiones relativas al espacio disponible, por ejemplo: \texttt{[width=0.5$\backslash$textwidth]} para conseguir que la figura ocupe la mitad del ancho de página y se mantenga la proporción alto/ancho. Dentro de las llaves se indicará el nombre del archivo que contiene la imagen.

Dado que aparte de la imagen en sí la figura ha de contar también con un pie o título, definido con el comando \verb|\caption|, y además es habitual asociar una etiqueta a la figura para así poder hacer referencia a ella desde el texto, usando el comando \verb|\label| que se explicó anteriormente, es preciso agrupar todos estos elementos dentro de un ámbito de tipo \texttt{figure}. Por ejemplo, el código del Listado~\ref{List.Figura} daría lugar a la inserción de la Figura~\ref{Fig.LogoUJA}. El comando \verb|\centering|, introducido dentro del ámbito \texttt{figure}, hace que la figura aparezca centrada en lugar de alineada a la izquierda.


\begin{lstlisting}[language={[LaTeX]TeX},caption={Insercción de una figura en el documento},label=List.Figura]
\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.4\textwidth]{imagenes/uja.jpg}
  \caption{Logo oficial de la Universidad de Jaen.}
  \label{Fig.LogoUJA}
\end{figure}
\end{lstlisting}


\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.4\textwidth]{imagenes/uja.jpg}
  \caption{Logo oficial de la Universidad de Jaén.}
  \label{Fig.LogoUJA}
\end{figure}

Las referencias desde el texto a la figura se generarían según lo explicado en la subsección~\textit{\ref{Sec.Referencias}~\nameref{Sec.Referencias}}.

\section{Cómo insertar tablas}

Las tablas son un elemento también muy habitual en una memoria de TFG, especialmente si de tipo experimental y, en consecuencia, ha de informar sobre resultados obtenidos de los experimentos realizados. Las tablas probablemente sean el tipo de elemento más complejo de \LaTeX por su flexibilidad, ya que pueden tener distribuciones no regulares y contener cualquier elemento en cada celdilla, incluso otras tablas.

\subsection{Aspectos importantes al usar tablas}

Las normas a tener presentes a la hora de introducir tablas en la memoria son las indicadas a continuación:

\begin{itemize}
    \item Todas las tablas han de estar numeradas, usando una numeración consecutiva para toda la memoria o bien relativa a cada capítulo.
    
    \item Todas las tablas han de contar con un pie de tabla que describa claramente qué información facilita. No ocurre nada si el pie de tabla es extenso y ocupa varias líneas. Es especialmente importante describir qué hay en cada columna en caso de que los títulos de la primera fila no sean autoexplicativos\footnote{En ocasiones conviene poner un título de columna corto a las columnas, para conseguir que el ancho de la tabla no exceda del ancho de la página, usándose el pie para facilitar una descripción más pormenorizada.}
    
    \item Todas las tablas han de estar referenciadas desde el texto de la memoria, al igual que las figuras.
    
    \item El formato de todas las tablas introducidas en la memoria será homogéneo, constando de:
    \begin{enumerate}
        \item Una línea horizontal que da inicio a la tabla
        \item Una línea con los encabezados de cada columna en negrita
        \item Una línea horizontal de separación entre encabezados y contenido
        \item Tantas líneas como filas deba contener la tabla
        \item Una línea horizontal de cierre de la tabla
        \item El pie de tabla
    \end{enumerate}
\end{itemize}

Al igual que las figuras o los títulos de sección y subsección, las tablas permiten \textit{romper} una sucesión de múltiples párrafos de texto haciendo más fácil la lectura de la memoria.

\subsection{Cómo almacenar las tablas}

Las tablas pueden introducirse directamente en el código fuente \LaTeX, pero en ocasiones puede interesar tenerlas almacenadas en archivos independientes, por ejemplo si han sido generadas por otro programa. En ese caso se guardarán en un archivo con extensión \texttt{.tex} y se incluirán en el capítulo que corresponda usando el comando \verb|\input{ruta/archivo.tex}|.

\subsection{Cómo insertar las tablas en el documento}

El ámbito para incluir una tabla en la memoria es \verb|tabular{}|\footnote{Recuerda que al ser un ámbito deberá ir delimitado con los comandos \texttt{$\backslash$begin} y \texttt{$\backslash$end}.}. Dentro de las llaves se indicará el número de columnas con que contará la tabla y su alineación, usando para ello los siguientes indicadores:

\begin{itemize}
    \item \texttt{l}: columna de longitud variable con alineación a la izquierda
    \item \texttt{r}: columna de longitud variable con alineación a la derecha
    \item \texttt{c}: columna de longitud variable con alineación al centro
    \item \texttt{p\{N.Mcm\}}: columna de longitud fija \texttt{N.M} centímetros
\end{itemize}

El ancho de la tabla será la suma de los anchos de sus columnas. Debe tenerse en cuenta que ese ancho se extenderá todo lo necesario para acoger su contenido salvo para columnas de tipo \texttt{p}.

Dentro del ámbito \texttt{tabular} incluiremos los siguientes elementos por este orden:

\begin{enumerate}
    \item \verb|\toprule|: una línea horizontal que delimita el inicio de la tabla
    \item \textbf{Encabezado}: una fila de encabezado con los títulos de las columnas en negrita. Cada columna se separará de la siguiente con el carácter \texttt{\&} y el final de la fila se marcará con \texttt{$\backslash\backslash$}
    \item \verb|\midrule|: una línea horizontal que separa el encabezado del contenido de la tabla
    \item \textbf{Contenido}: tantas filas de contenido como sean precisas, separando el dato de cada columna del de la siguiente con el carácter \texttt{\&} y marcando el final de cada fila con \texttt{$\backslash\backslash$}
    \item \verb|\bottomrule|: una línea horizontal para marcar el final de la tabla
\end{enumerate}

Dado que la tabla debe contar además con un título y también un etiqueta, como las figuras, será preciso usar los comandos \verb|\caption| y \verb|\label| ya explicados. Todos ellos se unirán en un entorno \verb|table|, tal y como muestra el ejemplo del Listado~\ref{List.Tabla} que produce como resultado la Tabla~\ref{Tabla.Requisitos}.

\begin{lstlisting}[language={[LaTeX]TeX},caption={Creación de una tabla},label={List.Tabla}]
\begin{table}[ht!]
  \centering
  \small
  \def\arraystretch{1.5}
  \begin{tabular}{llp{10cm}}
    \toprule
      \textbf{Componente} & \textbf{Requerimiento} & \textbf{Descripcion} \\
    \midrule
      Procesador & Arquitec. x64  & Procesador con varios (\textit{cores}) para poder ejecutar los distintos contenedores que forman el sistema \\
      
      RAM        & 64GB  & Disponer de suficiente memoria RAM es importante para garantizar un funcionamiento correcto del sistema \\
      
      Disco      & 10TB  & Almacenamiento masivo suficiente para contener los datos. Se recomienda una velocidad de al menos 7200rpm \\
      
      GPU        & CCC $\ge$ 3.5 & GPU para juegos con CUDA Compute Capability 3.5 o posterior y al menos 8GB de memoria \\
      
      Red        & GbE & Gigabit Ethernet para el acceso del sistema a web \\
    \bottomrule
  \end{tabular}
  \caption{Requisitos hardware del ordenador }
  \label{Tabla.Requisitos}
\end{table}
\end{lstlisting}

En este ejemplo se han usado, además de los ya descritos, algunos comandos \LaTeX~ adicionales: con \verb|\centering| se consigue que la tabla, si no ocupa todo el ancho de la página, aparezca centrada; con \verb|small| se ajusta el tipo de letra de la tabla para que sea más pequeño que el texto general y, de esta forma, pueda ajustarse al espacio disponible; por último con \verb|\def\arraystrectch{1.5}| se modifica la separación por defecto de las líneas de la tabla, que es \texttt{1.0}, incrementándola a \texttt{1.5}.

\input{tablas/TablaEjemplo.tex}

Es habitual que las tablas y figuras, cuando su tamaño es mayor que el espacio que resta en la página, pasen automáticamente al inicio de la página siguiente. \LaTeX siempre tratará de ajustar los contenidos para conseguir el mejor resultado posible, de forma que el texto quede ajustado llenando las páginas. Este párrafo de texto, por ejemplo, está dispuesto tras la Tabla~\ref{Tabla.Requisitos}, pero en el documento aparece dividido rodeando a la tabla, de forma que la página anterior no quede con un espacio vacío en la parte inferior. Puedes probar a mover párrafos de texto, poniéndolos delante o detrás del punto en el que se introduce la tabla/figura, para conseguir la disposición que te interese.

\section{Cómo insertar fórmulas/ecuaciones}

\LaTeX~ se caracteriza por su versatilidad a la hora de tratar con símbolos, fórmulas y ecuaciones matemáticas, consiguiendo unos resultados de calidad superior. Esencialmente nos encontraremos con dos escenarios de uso distintos: 

\begin{itemize}
    \item La introducción de expresiones matemáticas en línea con el texto existente en un párrafo. En este caso se usarán el delimitador \texttt{\$} al inicio y final de la expresión, usando en su interior los comandos específicos de \LaTeX para esta tarea. Por ejemplo, con la expresión \verb|$f(n) = n^5 + 4n^2 + 2$| conseguimos el resultado $f(n) = n^5 + 4n^2 + 2$.
    
    \item La introducción de ecuaciones de manera independiente, en cuyo caso deben ir numeradas y se les asignará una etiqueta para poder hacer referencia a ellas desde el texto. Con este fin se usará el ámbito \verb*|equation|. La numeración se generará automáticamente y aparecerá en el margen derecho, entre paréntesis.
\end{itemize}

El código mostrado en el Listado~\ref{List.Ecuacion} genera como resultado la Ecuación~\ref{Eq.Prob1}. Observa el uso del comando \verb|\label| para asignar una etiqueta que se ha usado en este párrafo de texto para hacer referencia a ella con el comando \verb|\ref|.

\begin{lstlisting}[language={[LaTeX]TeX},caption={Definición de una ecuación},label=List.Ecuacion]
\begin{equation}
    P\left(A=2\middle|\frac{A^2}{B}>4\right)
    \label{Eq.Prob1}
\end{equation}
\end{lstlisting}

\begin{equation}
    P\left(A=2\middle|\frac{A^2}{B}>4\right)
    \label{Eq.Prob1}
\end{equation}

Sobre el lenguaje que permite generar las expresiones matemáticas se podría escribir un libro completo. En WikiBooks (\url{en.wikibooks.org/wiki/LaTeX/Mathematics}) puedes encontrar una introducción que te solventará la mayoría de dudas al respecto.

\section{Cómo insertar algoritmos}

En una memoria de TFG es habitual tener que describir cómo funcionan los algoritmos usados o propuestos. Con ese fin lo más recomendable es facilitar un pseudo-código que describa dichos algoritmos. Es deseable que todos los algoritmos se describan con una sintaxis y formato similares, de forma que sean consistentes a lo largo de toda la memoria.

Por esa razón se recomienda usar el entorno \verb|algorithm| a la hora de describir algoritmos. En este entorno usaremos los comandos \verb|\caption| y \verb|\label|, como haríamos con una figura o una tabla, y aparte un conjunto de comandos específicos para la definición de condicionales, bucles, etc., tal y como se aprecia en el Algoritmo~\ref{Alg.AlgoritmoBasico}. Este ha sido generado con el código \LaTeX~ que aparece en el Listado~\ref{List.Algoritmo}.

\begin{algorithm}[H]
\SetAlgoLined
\KwResult{Resultado que devuelve algoritmo}
 inicialización\;
 \While{condición}{
  instrucciones\;
  \eIf{condición}{
   instrucciones1\;
   instrucciones2\;
   }{
   instrucciones3\;
  }
 }
 \caption{Título del algoritmo descrito}
 \label{Alg.AlgoritmoBasico}
\end{algorithm}

Observa que el título del algoritmo aparece en la parte superior, en lugar de en la inferior como ocurre con imágenes, tablas y otros elementos. Al igual que en las tablas, se usan líneas horizontales que delimitan las distintas partes del algoritmo.

\begin{lstlisting}[language={[LaTeX]TeX},caption={Descripción de un algoritmo},label=List.Algoritmo]
\begin{algorithm}[H]
\SetAlgoLined
\KwResult{Resultado que devuelve algoritmo}
 inicializacion\;
 \While{condicion}{
  instrucciones\;
  \eIf{condicion}{
   instrucciones1\;
   instrucciones2\;
   }{
   instrucciones3\;
  }
 }
 \caption{Titulo del algoritmo descrito}
\end{algorithm}
\end{lstlisting}

En cuanto a los comandos que pueden emplearse dentro del entorno \verb|algorithm|, encontrarás información de referencia y abundancia de ejemplos en el manual del paquete \LaTeX~ \texttt{algorithm2e} (\url{osl.ugr.es/CTAN/macros/latex/contrib/algorithm2e/doc/algorithm2e.pdf}), encargado de aportar el citado entorno y sus comandos específicos. En la mayoría de los casos solo precisarás usar un pequeño subconjunto de esos comandos para describir el algoritmo.

\section{Cómo insertar código}

Además de algoritmos descritos desde una perspectiva abstracta, mediante pseudo-código, también es probable que en la memoria sea preciso introducir listados de código. En las secciones previas aparecen múltiples listados, todos ellos han sido generados con el entorno \verb|lstlisting|. Este toma, entre paréntesis, tres parámetros:

\begin{itemize}
    \item \texttt{caption}: establece el título que aparecerá bajo el listado
    \item \texttt{language}: indica el lenguaje usado en el listado, de forma que se diferencien con colores y tipos de letra las distintas partes
    \item \texttt{label}: etiqueta a asignar para referenciar el listado
\end{itemize}

En el Listado~\ref{List.SQL} se muestra el fuente \LaTeX~ usado para producir el Listado~\ref{LstCreaTabla}. En el primero faltaría cerrar el entorno con el comando \verb*|\end{lstlisting}|.

\begin{lstlisting}[caption={Fuente LaTeX que genera el listado en lenguaje SQL},language={[LaTeX]TeX},label=List.SQL]
\begin{lstlisting}[caption={Crear una tabla},language=SQL,label=List.CreaTabla]
CREATE TABLE Entradas(
    id INTEGER PRIMARY KEY,
    asiento INTEGER,
    cliente CHAR(50)
);
\end{lstlisting}

\begin{lstlisting}[caption={Creación de una tabla con SQL},language=SQL,label=LstCreaTabla]
CREATE TABLE Entradas(
    id INTEGER PRIMARY KEY,
    asiento INTEGER,
    cliente CHAR(50)
);
\end{lstlisting}

El paquete que facilita el entorno \verb|lstlisting| contempla el uso de decenas de lenguajes. La lista completa la tenemos en \url{en.wikibooks.org/wiki/LaTeX/Source_Code_Listings}, de donde tomaríamos el nombre que es necesario asignar al parámetro \texttt{language} del entorno. En esa misma página también se describe cómo definir una configuración a medida para lenguajes que no estén soportados inicialmente. El formato por defecto para esta plantilla está definido en el archivo \texttt{Portada.tex}, junto con muchas otras definiciones.

\section{Cómo citar textos y otros materiales}

Durante el desarrollo del TFG es habitual recurrir al estudio de diversos materiales, entre los que se incluyen libros, artículos científicos y, por supuesto, páginas web con tutoriales e instrucciones. También durante la redacción de la memoria es posible que se usen definiciones tomadas literalmente o incluso diagramas y figuras.

En todos esos casos es imprescindible citar de manera adecuada la fuente de la que procede la información en que basamos nuestro trabajo o que reproducimos literalmente en la memoria. 

\subsection{Citas bibliográficas}

Cualquier libro, artículo, contribución a congreso o recurso web que hayamos consultado para poder adquirir los conocimientos necesarios para desarrollar el TFG, ya sean teóricos sobre técnicas o prácticos sobre herramientas, deberíamos citarlos en nuestra memoria. La introducción de estas citas es un aspecto que se descuida en muchas ocasiones y que suele afectar negativamente a la evaluación de una memoria.

La introducción de citas bibliográficas en la memoria se lleva a cabo mediante el comando \verb|\cite{}|, introduciendo en las llaves el identificador que se haya asignado a la referencia bibliográfica en el archivo \texttt{bibliografía.bib}. El proceso consta de varios pasos:

\begin{enumerate}
    \item Obtener los datos bibliográficos en formato BibTeX:
    \begin{itemize}
        \item En Google Scholar (\url{scholar.google.es}) hacer clic en el icono en forma de comillas que aparece debajo del título del libro/artículo, a continuación elegir la opción \textbf{BibTeX}, seleccionar todo y copiar al portapapeles.
        
        \item La revista/editorial en que se haya publicado el trabajo suele ofrecer la información bibliográfica en formato BibTeX.
        
        \item Crear manualmente la entrada BibTeX con los datos bibliográficos que hayamos obtenido de la fuente de la que procede el material.
    \end{itemize}
    
    \item Introducir los datos bibliográficos en el archivo \texttt{bibliografía.bib}, asignando en la primera línea un identificador que nos sea fácil recordar por el tema y/o autor del material.
    
    \item Insertar en la ubicación que proceda de nuestra memoria la referencia bibliográfica, usando para ello el comando \verb|\cite{}| y el identificador que se haya asignado en el paso previo.
\end{enumerate}

Las entradas BibTeX almacenadas en el archivo \texttt{bibliografía.bib} tendrán un formato u otro dependiendo de que el material a citar sea un artículo científico, un libro, una contribución a un congreso o un recurso web. Cada entrada BibTeX se compone de múltiples atributos con datos:

\begin{itemize}
    \item  Por regla general, toda entrada deberá contar al menos con los atributos \texttt{title}, \texttt{author} y \texttt{year}, con los que se facilita el título del trabajo, su autor y año de publicación. 
    
    \item En el caso de los libros también se debe incluir la editorial e ISBN, usando para ello los atributos \texttt{publisher} e \texttt{isbn}, respectivamente.
    
    \item Para los artículos indicar el volumen, número y páginas que ocupa en la revista, así como el nombre de esta, usando los atributos \texttt{volume}, \texttt{number}, \texttt{pages} y \texttt{journal}, respectivamente.
    
    \item En el caso de los recursos web es indispensable facilitar en el campo \texttt{url} la dirección web, así como indicar en el atributo \texttt{note} la fecha en que se verificó la disponibilidad del recurso.
    
    \item Siempre que sea posible, usar el atributo \texttt{doi} para facilitar el DOI (\textit{Digital Object Identifier}) del documento.
\end{itemize}

Debemos tener presente que algunos servicios, como el mencionado Google Scholar, no facilita información completa en las entradas BibTeX que generan automáticamente. Por ello siempre debemos revisarlas y, en caso necesario, completar los datos que falten, buscándolos en el editor del libro, página web de la revista, etc.

A modo de ejemplo, en el Listado~\ref{List.BibTeX} se facilitan cuatro referencias bibliográficas, cada una de ellas correspondiente a uno de los tipos de entradas BibTeX mencionados. Estos ejemplos también los encontrarás en el archivo \texttt{bibliografía.bib}, por lo que puedes usarlos como plantilla para crear tus propias entradas en caso necesario.

\begin{lstlisting}[caption={Entradas BibTex de distintos tipos},label=List.BibTeX,language={[LaTeX]TeX}]
@article{UnArticulo,
  title={A Comprehensive and Didactic Review on Multilabel Learning Software Tools},
  author={Charte, Francisco},
  journal={IEEE Access},
  volume={8},
  pages={50330--50354},
  year={2020},
  publisher={IEEE},
  doi={10.1109/ACCESS.2020.2979787}
}

@book{UnLibro,
  title={Multilabel Classification: Problem Analysis, Metrics and Techniques},
  author={Herrera, Francisco and Charte, Francisco and Rivera, Antonio J and del Jesus, Mar{\'\i}a J},
  year={2016},
  publisher={Springer},
  ISBN={978-3-319-41110-1},
  doi={10.1007/978-3-319-41111-8}
}

@inproceedings{UnCongreso,
  title={A first approach to deal with imbalance in multi-label datasets},
  author={Charte, Francisco and Rivera, Antonio and del Jesus, Mar{\'\i}a Jos{\'e} and Herrera, Francisco},
  booktitle={International Conference on Hybrid Artificial Intelligence Systems},
  pages={150--160},
  year={2013},
  organization={Springer},
  doi={10.1007/978-3-642-40846-5\_16}
}

@online{UnaWeb,
  author = {Charte, Francisco},
  title = {Cómo analizar la distribución de los datos con R},
  year = 2019,
  url = {https://fcharte.com/tutoriales/20170114-DistribucionDatosR/}, 
  note = {comprobado en 2020-09-30}
}
\end{lstlisting}

Teniendo las entradas BibTeX ya preparadas, solo quedaría introducir las referencias en los puntos adecuados del texto, tal y como se explicó antes. Un párrafo como el mostrado en el Listado~\ref{List.Citas} generaría el resultado que se aprecia justo a continuación. Además, introducirá la información bibliográfica completa en la lista de referencias, al final de la memoria, con un formato adecuado.

\begin{lstlisting}[caption={Párrafo en el que se citan varios trabajos},language={[LaTeX]TeX},label=List.Citas]
Como se explica en~\cite{UnArticulo} y se amplía en el libro~\cite{UnLibro}, las técnicas 
que se presentaron en~\cite{UnCongreso} pueden ponerse en práctica siguiendo el 
tutorial~\cite{UnaWeb}
\end{lstlisting}

\begin{quotation}
Como se explica en~\cite{UnArticulo} y se amplía en el libro~\cite{UnLibro}, las técnicas que se presentaron en~\cite{UnCongreso} pueden ponerse en práctica siguiendo el tutorial~\cite{UnaWeb}
\end{quotation}

\subsection{Citas textuales}

Si además de emplear un material como base para el estudio y desarrollo de nuestro TFG, caso en el que lo citaremos según lo que acaba de explicarse, también hemos tomado alguna parte del mismo para introducirlo literalmente en la memoria, hemos de tener en cuenta las siguientes normas:

\begin{itemize}
    \item Únicamente pueden citarse textualmente pequeñas porciones de un trabajo, habitualmente no más de un párrafo y en ningún caso páginas completas.
    
    \item El texto citado ha de aparecer claramente diferenciado en el texto de la memoria, para lo cual usaremos habitualmente el entorno \verb|\displayquote| y pondremos el texto en cursiva, consiguiendo un resultado como el siguiente:
    
    \begin{displayquote}
    \textit{Learning, like intelligence, covers such a broad range of processes that it is difficult to define precisely. A dictionary definition includes phrases such as “to gain knowledge, or understanding of, or skill in, by study, instruction, or experience,” and “modification of a behavioral tendency by experience.” Zoologists and psychologists study learning in animals and humans. In this book we focus on learning in machines.
    }\end{displayquote}
    
    \item La cita textual ha de ir acompañada de la correspondiente cita bibliográfica, de forma que sea posible determinar el origen del material de manera inequívoca.
    
    \item En general no es recomendable incluir en la memoria ninguna ilustración, diagrama o figura cuyos derechos de uso no estén claros. Si la licencia de uso es \textit{Creative Commons} se puede incluir la figura indicando tanto la licencia como la procedencia. En cualquier otro caso es necesario obtener permiso del propietario para la reproducción del material. La alternativa es construir nuestra propia ilustración o diagrama indicando que se ha tomado como base una fuente existente y facilitando la correspondiente referencia.
\end{itemize}

Ten siempre presente que tu memoria de TFG va a publicarse y cualquiera tendrá acceso al trabajo que has hecho durante mucho tiempo. También es habitual que la memoria se sometida a herramientas de detección de plagio. Por todo ello, es importante ser honesto y no usar nunca trabajo de terceros sin citarlo apropiadamente.

\section{Cómo introducir ciertos caracteres en \LaTeX}

Al trabajar con fuentes \LaTeX~ empleamos un conjunto de caracteres que tienen un significado especial, como es el carácter $\backslash$ para con el que se inician los comandos, el carácter \$ que delimita las expresiones matemáticas, el carácter \% para introducir comentarios, los símbolos \_ y \^~ para escribir subíndices y superíndices en fórmulas matemáticas, etc. Incluso algunos símbolos, como es el caso de las "comillas dobles", pueden tener un resultado no deseado\footnote{En el fuente \LaTeX~ se ha usado el carácter '' para iniciar el entrecomillado, pero esto ha tenido como efecto convertir la ''c'' inicial en una "c.} en el documento.

En general, usaremos el símbolo $\backslash$ como carácter de escape, para que el carácter que dispongamos a continuación aparezca como tal en el texto. No obstante hay excepciones: para incluir una barra invertida no podemos usar $\backslash\backslash$, porque eso indica un salto de línea.

Si tienes problemas para introducir algún carácter puedes recurrir a \url{en.wikibooks.org/wiki/LaTeX/Special_Characters} para saber cómo debes introducirlo en tu memoria.

\section{Cómo obtener esta plantilla}

\section{Cómo configurar la plantilla con nuestros datos}

A fin de generar nuestra memoria de TFG usando esta plantilla, aprovechando toda la configuración de formatos ya adaptada a la normativa de la EPSJ, tendremos que comenzar abriendo el archivo \texttt{main.tex} para configurar nuestros datos personales. Para ello localiza el bloque \LaTeX~ que aparece en el Listado~\ref{List.Variables} y sigue las instrucciones indicadas en los comentarios.

\begin{lstlisting}[caption={Variables a establecer en \texttt{main.tex}},language={[LaTeX]TeX},label=List.Variables]
% ==== Introducir aquí el nombre del estudiante
\def\Estudiante{Nombre1 Nombre2 Apellido1 Apellido2}

% ==== Introducir aquí el nombre de los tutores. Si solo hay uno dejar las llaves de $\backslash$TutorB vacías
\def\TutorA{Nombre y apellidos del tutor 1}
\def\TutorB{Nombre y apellidos del tutor 2}

% ==== Introducir aquí el título de completo y abreviado (para las cabeceras) del TFG
\def\TituloTFG{El título del TFG tal y como aparece en la propuesta original}
\def\TituloAbreviado{Título abreviado para el encabezado}

% ==== Introducir aquí el mes y año de presentación del TFG
\def\Fecha{junio de 2021}
\end{lstlisting}

Solo necesitas establecer el contenido de estas variables para que la portada, página de autorización y los encabezados muestren los datos correctos. Habitualmente esto es todo lo que necesitarás cambiar en el archivo \texttt{main.tex}. No obstante, si no quieres incluir al inicio de la memoria alguno de los índices que aparecen por defecto, como los de algoritmos, listados, etc., también deberás eliminar o comentar (colocar delante el carácter \%) las líneas correspondientes.

\section{En cuanto al contenido de los capítulos}

Una vez que hayas establecido la configuración con tus datos, el paso siguiente será escribir el contenido de la memoria, completando los capítulos que se han previsto en la plantilla. Cada uno de ellos está almacenado en un archivo independiente y se inserta desde \texttt{main.tex} usando el comando \verb|\input|. 

Teniendo presente que la estructura de esta memoria se corresponde con la de un TFG de tipo experimental, los capítulos y su contenido son los siguientes:

\begin{enumerate}
    \item \textbf{Introducción} - \texttt{Introduccion.tex}\footnote{En la plantilla este archivo contiene las instrucciones de cómo usar \LaTeX~ y la propia plantilla, por lo que deberás eliminar dicho contenido o bien crear tu propio archivo \texttt{Introduccion.tex} desde cero.} \par
    La introducción al TFG, como su propio nombre denota, ha de servir como un acercamiento general y desde una perspectiva de alto nivel al trabajo realizado. Debe explicarse la motivación que nos ha llevado a abordar el problema en cuestión, su importancia, cómo se ha tratado hasta le momento, qué creemos que podemos aportar, etc., todo ello sin entrar demasiado en detalle. Es habitual indicar en la parte final de la introducción la estructura de la memoria, enumerando sus capítulos con una breve descripción de lo que se explica en cada uno de ellos. También puedes incluir un índice o tabla, al final de este capítulo, conteniendo todos los acrónimos y términos que uses en la memoria, de forma que sea fácil para quien la lea saber qué significan\footnote{La alternativa es definirlos la primera vez que aparezcan en el texto, por ejemplo: «En este TFG (\textit{Trabajo Fin de Grado}) se aborda ...».}.
    
    \item \textbf{Antecedentes} - \texttt{Antecedentes.tex} \par
    Este capítulo tiene como objetivo demostrar que has estudiado el tema que se aborda en el TFG, describiendo primero el problema con todos los detalles necesarios y después las técnicas que se han empleado hasta el momento para tratarlo y las que propones usar tú. Un aspecto fundamental de este capítulo es la bibliografía: has de recopilar los artículos científicos más relevantes sobre el tema en cuestión, consultar libros sobre las técnicas a usar, etc., referenciando todo ello de manera adecuada.
    
    \item \textbf{Objetivos} - \texttt{Objetivos.tex} \par
    El título del capítulo lo dice todo: ¿cuáles son los objetivos que se persiguen al desarrollar este TFG? Habitualmente se expondrá un objetivo general, desde una perspectiva de alto nivel, y después se descompondrá en tantos objetivos específicos como sea preciso, detallando cada uno de ellos al máximo. En teoría, solo leyendo este capítulo debería obtenerse una idea muy clara de qué va a hacerse (se ha hecho) en el TFG.
    
    \item \textbf{Materiales y métodos} - \texttt{MaterialMetodos.tex} \par
    Para alcanzar los objetivos establecidos en el capítulo previo será preciso usar unos materiales: por ejemplo los datos que servirán para generar los modelos, y también unos métodos: las técnicas y herramientas que servirán para obtener los resultados. Todos esos aspectos han de quedar reflejados en este capítulo hasta el más mínimo detalle, explicando, por ejemplo, cómo se han obtenido los datos, si ha sido necesario prepararlos de alguna manera antes de poder usarlos; qué técnicas/algoritmos van a emplearse y por qué, cómo va a realizarse la ejecución (configuración hardware y software), etc.
    
    \item \textbf{Resultados} - \texttt{Resultados.tex} \par
    Como salida de aplicar los métodos a los materiales se obtienen resultados. La finalidad de este capítulo doble: por una parte se deben reflejar todos los resultados obtenidos, usualmente en tablas en gráficas, por otra, ha de efectuarse un análisis pormenorizado de dichos resultados a fin de determinar su valor. Siempre que sea posible, deberían realizarse comparaciones de resultados entre distintos métodos, ya los hayamos aplicado nosotros o hayan sido previamente publicados en la bibliografía citada en los antecedentes.
    
    \item \textbf{Conclusiones} - \texttt{Conclusiones.tex} \par
    El último capítulo de la memoria ha de presentar una recopilación de todo el trabajo realizado, los resultados obtenidos y un análisis que determine, y deje claro, si se han alcanzado las metas que se establecieron en la propuesta inicial del TFG que aprobó la correspondiente comisión. Además, también es habitual incluir una sección describiendo potenciales mejoras que podrían efectuarse o trabajos futuros que podrían derivarse de lo hecho en el TFG.
\end{enumerate}

Además de los archivos anteriores, correspondientes a los capítulos, también debes editar el contenido del archivo \texttt{Agradecimientos.tex} para escribir tus propios agradecimientos y dedicatoria.

\section{Consejos sobre redacción}

Para terminar estas instrucciones, y aunque no tiene que ver con la plantilla \LaTeX~ ni aspectos relativos a la normativa de realización de TFG de la EPSJ, quiero incidir en un aspecto de vital importancia: la calidad de la redacción.

Tanto para el tribunal que evaluará tu TFG como para las personas que puedan leerlo en el futuro, la corrección del texto será siempre un reflejo de la calidad del trabajo que has llevado a cabo, aunque esta no sea la realidad. Es habitual que un muy buen trabajo vea su evaluación penalizada a causa de una pobre redacción de la memoria. Por el contrario, un trabajo menos brillante pero con una presentación perfecta en la memoria puede ser mejor valorado.

Sin ánimo de ser exhaustivo, porque tampoco es la finalidad de estas instrucciones, he aquí una lista de aspectos a tener presentes en la redacción:

\begin{itemize}
    \item \textbf{Ortografía:} posiblemente no haya nada que cause una peor impresión al leer una memoria que encontrar errores de ortografía. Extrema el cuidado en este sentido: revisa las normas de uso de $b$ y $v$, de $g$ y $j$, de $m$ y $n$, de uso de la $h$, las relativas a las tildes, etc. Tan malo es el defecto como el exceso, por ejemplo poniendo tilde en términos como los pronombres ($este$, $esta$, $estas$) que la normativa indica desde hace años que no deben llevar tilde. Ante la duda usa el Diccionario de la RAE (\url{buscon.rae.es}) y las consultas de la Fundéu (\url{www.fundeu.es/consultas}).
    
    \item \textbf{Gramática:} usar los tiempos verbales correctos, respetar las concordancias de género y número y otros aspectos gramaticales es también vital para facilitar la correcta comprensión de las explicaciones dadas en la memoria. Una de estas normas nos dice que a los acrónimos y abreviaciones no se les añade la $s$ final para formar el plural, sino que el número lo denota el determinante o el contexto, por ejemplo «En los TFG de este curso ...», en lugar de «En los TFGs de este curso ...». Otra regla es la de no comenzar con mayúsculas tras dos puntos, salvo para nombres propios, tal y como ves en esta misma sección, en la que tras el título en negrita y los dos puntos se continúa usando minúsculas.
    
    \item \textbf{Vocabulario:} el vocabulario usado al redactar denota diversas capacidades por parte del autor de la memoria. En primer lugar, el uso de términos especializados, específicos y exactos, del campo en el que se está trabajando indica que se conoce la materia, se ha leído sobre el tema. En segundo, el uso de un vocabulario variado, evitando repetir las mismas palabras y términos en un corto espacio, indica riqueza en el uso del lenguaje. Por último, debe tenerse presente que la memoria de un TFG es un documento formal y que, en consecuencia, debe evitarse el uso de términos demasiado coloquiales. Por ejemplo, usar «Según se ha descrito en ...» en lugar de «Como se ha comentado en ...».
    
    \item \textbf{Estructuración:} la memoria al completo, pero también cada uno de sus capítulos de manera aislada y cada sección de cada capítulo, deben redactarse como una relación de ideas interconectadas entre sí y que tienen sentido. Nada hay más desconcertante que leer una memoria en la que cada párrafo trata sobre ideas inconexas con lo que tiene a su alrededor, en párrafos anteriores y posteriores, provocando una sensación de que va saltándose de un tema a otro sin orden alguno. Por ello es importante llevar a cabo una planificación previa del contenido de cada capítulo y de cada sección, de forma que, al igual que durante el desarrollo de un software, se maximice la cohesión y se minimice el acoplamiento.
\end{itemize}

Aunque es algo totalmente fuera del ámbito de estas instrucciones, una última recomendación: la mejor forma de aprender a redactar correctamente estriba en leer mucho, especialmente literatura.