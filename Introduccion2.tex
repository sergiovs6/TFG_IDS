% !TeX spellcheck = es_ES

% Cada capítulo de la memoria de TFG comienza con \chapter{TÍTULO DEL CAPÍTULO}, tal y como requiere la normativa de la EPSJ
\chapter{INTRODUCCIÓN}  

El presente Proyecto de Fin de Grado aborda una de las problemáticas más acuciantes en el ámbito de la ciberseguridad: la detección proactiva de intrusiones en redes informáticas. En un panorama digital en constante evolución, donde las amenazas son cada vez más sofisticadas y persistentes, la capacidad de identificar y neutralizar actividades maliciosas en tiempo real se ha convertido en un pilar fundamental para la protección de la información y la infraestructura crítica. Tradicionalmente, los Sistemas de Detección de Intrusiones (IDS) se basaban en firmas o reglas predefinidas, un enfoque que, si bien eficaz contra amenazas conocidas, presenta limitaciones intrínsecas ante nuevos ataques o variaciones de los existentes.

Este trabajo propone el desarrollo de un Sistema de Detección de Intrusiones (IDS) innovador, que fusiona la captura y el monitoreo de tráfico de red en tiempo real con la potencia de las técnicas de aprendizaje automático (Machine Learning). El objetivo principal es la creación de un modelo de clasificación robusto y preciso, capaz de discernir de forma autónoma si un determinado flujo de tráfico de red es benigno (legítimo) o malicioso (indicativo de una intrusión o ataque). Esta aproximación busca superar las deficiencias de los sistemas basados en firmas, ofreciendo una mayor adaptabilidad y capacidad para detectar anomalías y comportamientos nunca antes vistos.

Para lograr este cometido, se hará uso de diversas tecnologías y metodologías de vanguardia. La fase de captura y preprocesamiento de datos de red se implementará mediante una aplicación diseñada específicamente para tal fin, garantizando la obtención de información relevante y en el formato adecuado para su análisis. Posteriormente, esta información será alimentada a un modelo de Machine Learning, concretamente un algoritmo de Random Forest (Bosques Aleatorios), seleccionado por su probada eficacia, robustez y capacidad para manejar grandes volúmenes de datos con alta dimensionalidad, características esenciales en el análisis de tráfico de red.

La estructura de este primer capítulo sentará las bases conceptuales y técnicas de todo el proyecto. A través de una descripción detallada de los componentes clave y la visión general del sistema propuesto, se facilitará la comprensión de los antecedentes del trabajo, los cuales serán explorados en profundidad en el segundo capítulo de esta memoria. Este enfoque metodológico permite establecer un marco claro para la posterior justificación, diseño e implementación de la solución planteada, enfatizando la relevancia y el impacto potencial de un IDS basado en aprendizaje automático en el contexto actual de la ciberseguridad.

\section{Fundamentos}

En este capítulo se establecerán las bases teóricas y conceptuales fundamentales que sustentan el análisis, diseño e implementación del sistema de detección de intrusiones propuesto. Se abordará en profundidad el paradigma de los Sistemas de Detección de Intrusiones, las bases del aprendizaje automático como disciplina, y se hará un énfasis particular en el comprensión del algoritmo de Random Forest, pieza clave de la solución desarrollada. Finalmente, se detallarán las métricas esenciales para la evaluación rigurosa del rendimiento de modelos de clasificación en el contexto de la ciberseguridad. Se comenzará explicando el concepto de \textbf{ciberseguridad y amenazas en redes}.


\subsection{Ciberseguridad y amenazas en redes}

La seguridad de la información constituye un pilar fundamental en la sociedad digital contemporánea, donde la interconexión global y la dependencia de los sistemas informáticos son crecientes. La protección de los activos digitales se articula en torno a los principios de confidencialidad, integridad y disponibilidad (CID). La confidencialidad garantiza que la información sea accesible únicamente por entidades autorizadas; la integridad asegura que la información no ha sido alterada de forma no autorizada; y la disponibilidad se refiere a la capacidad de los usuarios autorizados para acceder a la información y los sistemas cuando sea necesario. Cualquier evento que comprometa uno o más de estos principios se clasifica como una amenaza, la cual, al explotar una vulnerabilidad existente, puede materializarse en un riesgo para la organización.

El panorama de amenazas en redes informáticas evoluciona constantemente, presentando un desafío dinámico para la ciberseguridad. Si bien históricamente los ataques se centraban en la explotación de vulnerabilidades conocidas o errores de configuración, la sofisticación actual se manifiesta en técnicas de evasión más complejas y la emergencia de amenazas persistentes avanzadas (APT). Entre los tipos de ataques más prevalentes que un sistema de detección de intrusiones debe ser capaz de identificar se encuentran:
\begin{itemize}

    \item\textbf{Ataques de Denegación de Servicio (DoS/DDoS)}: Dirigidos a agotar los recursos de un sistema o red, impidiendo el acceso a servicios legítimos.
    
    \item\textbf{Escaneo de Puertos y Reconocimiento}: Fases preliminares donde un atacante explora una red en busca de puntos débiles o servicios abiertos.
    
    \item\textbf{Ataques de Fuerza Bruta}: Intentos sistemáticos y repetitivos para adivinar credenciales de acceso o claves de cifrado.
    
    \item\textbf{Malware (Software Malicioso)}: Incluye virus, troyanos, ransomware y spyware, diseñados con propósitos destructivos, de espionaje o de control remoto.
    
    \item\textbf{Explotación de Vulnerabilidades}: Aprovechamiento de fallos de diseño o implementación en software y hardware para obtener acceso no autorizado o ejecutar código malicioso.
    La capacidad de identificar estos y otros comportamientos anómalos es crucial para mantener la postura de seguridad de una infraestructura de red.

\end{itemize}

\subsection{Sistema de detección de instrusiones}\label{Sec.Capitulos}

En respuesta a la creciente complejidad del panorama de amenazas, los Sistemas de Detección de Intrusiones (IDS) han emergido como componentes esenciales de una estrategia de defensa en profundidad. Un IDS puede definirse como una aplicación de seguridad que monitoriza el tráfico de red o la actividad de un sistema con el fin de identificar patrones o comportamientos indicativos de una intrusión o una violación de políticas de seguridad. Su función principal no es bloquear el ataque (labor de un IPS o firewall), sino generar alertas que permitan a los administradores de seguridad tomar las medidas correctivas oportunas.

La clasificación de los IDS se realiza habitualmente atendiendo a su método de detección y a su ubicación en la infraestructura:
\begin{itemize}
    
    \item\textbf{IDS Basados en Firmas (Signature-based IDS - SIDS)}: Estos sistemas operan mediante la comparación del tráfico de red o eventos del sistema con una base de datos de firmas predefinidas, las cuales corresponden a patrones conocidos de ataques. Su principal ventaja reside en una alta precisión en la detección de ataques ya identificados y catalogados, con una baja tasa de falsos positivos en esos escenarios. No obstante, su limitación inherente radica en la incapacidad para detectar ataques novedosos o variantes polimórficas para las cuales no existe una firma en su base de datos, lo que exige una constante actualización de las mismas.
    
    \item\textbf{IDS Basados en Anomalías (Anomaly-based IDS - AVIDS)}: A diferencia de los SIDS, los AVIDS no dependen de firmas de ataques conocidos. En su lugar, construyen un perfil de comportamiento "normal" de la red, los usuarios o las aplicaciones. Cualquier desviación significativa de este perfil es considerada una anomalía y, por ende, una posible intrusión. La principal fortaleza de los AVIDS es su capacidad para detectar ataques "zero-day" (desconocidos previamente) y ataques sutiles que no encajan en patrones preestablecidos. Sin embargo, su principal desafío es la mayor propensión a generar falsos positivos, ya que cualquier cambio en el comportamiento normal (como la introducción de un nuevo servicio o una carga de tráfico inusual pero legítima) puede ser erróneamente clasificado como un ataque, requiriendo un ajuste continuo y afinado.
    
\end{itemize}

En cuanto a su ubicación, se distinguen los Network-based IDS (NIDS), que analizan el tráfico de red en puntos estratégicos de la infraestructura sin depender de los hosts individuales, y los Host-based IDS (HIDS), que monitorizan la actividad interna de un sistema operativo o aplicación específica. El sistema desarrollado en este proyecto se enmarca dentro de la categoría de NIDS, enfocándose en el análisis del tráfico de red.

\subsection{Clasificación}

La clasificación es una de las principales tareas dentro de la minería de datos.
Como se expresó previamente, el objetivo de esta tarea es predecir una etiqueta categórica para cada ejemplo de un conjunto de datos a partir de sus atributos conocidos \cite{hastie2009elements}.

Existen diversos tipos de clasificación. Entre los más destacados, podemos encontrar:

\begin{itemize}
    \item\textbf{Clasificación binaria}: en este problema concreto de clasificación, es necesario determinar a qué clase pertenece cada ejemplo de los datos, pero únicamente hay dos clases a elegir. Se suelen usar para problemas de carácter binario, como si una persona está sana o enferma o, en el contexto de esta memoria, si una conexión determinada es benigna o un ataque.
    
    \item\textbf{Clasificación multiclase}: al contrario que en el caso previo, se debe determinar la clase de cada ejemplo de entre un conjunto de clases de tamaño superior a dos. En este TFG, también se hará uso de la clasificación multiclase para tratar de determinar el tipo de ataque de una conexión que se sabe maliciosa.
    
    \item\textbf{Clasificación multietiqueta}: un caso particular dentro de la clasificación multiclase es la clasificación multietiqueta, ya que, en lugar de asignar una sola clase a cada ejemplo del conjunto de datos, se le deben asignar un conjunto de clases en función a las características de los datos, a menudo siendo posible que cada elemento tenga un número de clases asociadas distinto.

\end{itemize}

\subsection{Descubrimiento de conocimiento en bases de datos}

El Descubrimiento de Conocimiento en Bases de Datos (KDD) es un proceso sistemático y iterativo, no trivial, diseñado para la extracción de patrones válidos, novedosos, potencialmente útiles y comprensibles a partir de grandes volúmenes de datos. Representa la filosofía subyacente a la transformación de datos crudos en inteligencia accionable, siendo crucial en dominios donde la toma de decisiones basada en evidencia es primordial, como la ciberseguridad. En el contexto de un Sistema de Detección de Intrusiones (IDS) basado en aprendizaje automático, el KDD proporciona el marco metodológico para derivar el "conocimiento" que permite al sistema identificar comportamientos anómalos o maliciosos en el tráfico de red.

El proceso de KDD se articula a través de una secuencia de etapas interdependientes:

\textbf{Fases del Proceso KDD}
\begin{enumerate}

    \item\textbf{Selección}: Esta etapa inicial define y adquiere los datos relevantes del dominio de aplicación. Para un IDS, esto implica la selección de datasets de tráfico de red, como el CIC-IDS2018, que contengan una representación adecuada de comportamientos tanto benignos como maliciosos. La exhaustividad y representatividad de esta selección son fundamentales para la generalizabilidad del modelo resultante.
    
    \item\textbf{Preprocesamiento}: Considerada a menudo la fase más laboriosa y crítica, el preprocesamiento aborda la preparación de los datos brutos. Los datos de red, por su naturaleza, suelen ser ruidosos, incompletos, o inconsistentes. Esta etapa involucra:
    \begin{itemize}

        \item\textbf{Limpieza de Datos}: Eliminación o corrección de datos erróneos, duplicados, inconsistencias y manejo de valores ausentes.
        
        \item\textbf{Normalización/Estandarización}: Ajuste de las escalas de las características numéricas para evitar que aquellas con rangos de valores más amplios dominen en el análisis.
        
        \item\textbf{Transformación}: Conversión de datos a formatos adecuados para la minería. En el caso de tráfico de red, esto incluye la agregación de paquetes individuales en flujos de red y la extracción de características de esos flujos (ej., duración, número de paquetes, banderas TCP), elementos esenciales para el aprendizaje automático.
          
    \end{itemize}
    \item\textbf{Transformación}: Aunque intrínsecamente ligada al preprocesamiento, esta fase se centra en refinar la representación de los datos para la minería. La ingeniería de características es su componente clave, donde el conocimiento experto del dominio de red se aplica para derivar atributos más complejos y discriminatorios (ej., ratios de bytes por paquete, entropía de los puertos destino). El objetivo es crear un conjunto de características que optimicen la capacidad del algoritmo de Machine Learning para identificar patrones de intrusión.
    
    \item\textbf{Minería de Datos (Data Mining)}: Este es el corazón del proceso KDD, donde se aplican algoritmos computacionales para descubrir patrones ocultos, asociaciones, cambios significativos, desviaciones o estructuras significativas en los datos. No es simplemente una técnica, sino un conjunto de enfoques, tareas y técnicas aplicadas a los datos ya preprocesados:
    \begin{itemize}

    \item\textbf{Enfoques de Minería de Datos}:
        \begin{itemize}
        
            \item\textbf{Aprendizaje Supervisado}: Como se detalla en el punto de Transformación, implica el uso de datos etiquetados para predecir un resultado. La detección de intrusiones es, en esencia, un problema de clasificación supervisada, donde se predice si un flujo es benigno o malicioso.
            
            \item\textbf{Aprendizaje No Supervisado}: Se utiliza para encontrar estructuras o patrones en datos sin etiquetas previas (ej., clustering para agrupar tráficos similares, o detección de anomalías para identificar comportamientos que se desvían de lo normal sin una etiqueta de "ataque" explícita). Aunque tu proyecto se centra en lo supervisado, estos enfoques complementarios son relevantes en KDD.
            
            \item\textbf{Aprendizaje Semi-supervisado}: Combina datos etiquetados y no etiquetados, útil en escenarios donde el etiquetado es costoso.
            
        \end{itemize}

    \item\textbf{Tareas Típicas de Minería de Datos}:
        \begin{itemize}
        
            \item\textbf{Clasificación}: Asignar elementos a categorías predefinidas (ej., "ataque" o "normal"). Esta es la tarea principal de tu IDS.
            
            \item\textbf{Regresión}: Predecir un valor numérico continuo.
            
            \item\textbf{Agrupación (Clustering)}: Dividir un conjunto de datos en grupos (clusters) de elementos similares.
            
            \item\textbf{Asociación}: Descubrir reglas que describen relaciones entre elementos.
            
            \item\textbf{Detección de Anomalías/Outliers}: Identificar patrones que no se ajustan a un comportamiento esperado.
              
        \end{itemize}
    \item\textbf{Técnicas Aplicadas en este Proyecto}: En este trabajo, la fase de minería de datos se concreta en la aplicación del algoritmo Random Forest para la tarea de clasificación. Su capacidad para manejar un gran número de características y su robustez ante el ruido lo hacen idóneo para los complejos datasets de tráfico de red.
    \end{itemize}
    
    \item\textbf{Evaluación y Presentación}: La etapa final valida la significancia y utilidad de los patrones descubiertos. Los modelos de clasificación se evalúan utilizando métricas específicas (precisión, sensibilidad, F1-score, curva ROC) en un conjunto de datos de prueba independiente, asegurando que el conocimiento extraído sea generalizable y no un artefacto del entrenamiento. La interpretabilidad del modelo, aunque un desafío, es un objetivo deseable que permite a los analistas de seguridad comprender la razón de una detección. Finalmente, la presentación del conocimiento se realiza a través de informes, visualizaciones (como el dashboard de tu aplicación), que comunican de manera efectiva los insights a los usuarios finales.

\end{enumerate}
\subsection{Machine Learning}

El aprendizaje automático (Machine Learning - ML), una rama de la inteligencia artificial, ha transformado diversas disciplinas al dotar a los sistemas de la capacidad de aprender de datos y mejorar su rendimiento con la experiencia, sin ser programados explícitamente para cada tarea. En el contexto de la ciberseguridad, y específicamente en la detección de intrusiones, el ML ofrece un enfoque adaptativo que puede identificar patrones complejos en grandes volúmenes de tráfico de red, superando las limitaciones de los métodos tradicionales basados en reglas fijas.

El ciclo de vida de un modelo de Machine Learning generalmente comprende las siguientes fases interdependientes:

\begin{itemize}
    
    \item\textbf{Recopilación y Preprocesamiento de Datos}: Etapa crucial que implica la obtención, limpieza, transformación y estandarización de los datos. La calidad de los datos de entrada es determinante para el éxito del modelo.
    
    \item\textbf{Ingeniería de Características (Feature Engineering)}: Proceso de seleccionar, crear o transformar variables (features) a partir de los datos brutos que mejor representen la información subyacente y sean más relevantes para el problema de predicción.
    
    \item\textbf{Selección y Entrenamiento del Modelo}: Consiste en elegir el algoritmo de ML más adecuado y ajustar sus parámetros utilizando un subconjunto de datos (conjunto de entrenamiento) para que aprenda los patrones deseados.
    
    \item\textbf{Evaluación y Optimización del Modelo}: Se mide el rendimiento del modelo con un conjunto de datos no visto (conjunto de validación o prueba) y se realizan ajustes de hiperparámetros para mejorar su eficacia.
    
    \item\textbf{Despliegue y Monitorización}: Una vez validado, el modelo se integra en un entorno de producción para realizar predicciones en tiempo real, siendo fundamental un monitoreo continuo de su desempeño.

\end{itemize}

Dentro del ML, el aprendizaje supervisado es el paradigma central para la detección de intrusiones basada en clasificación. En este enfoque, el modelo aprende de un conjunto de datos que incluye tanto las características de entrada (ej., métricas del tráfico de red) como sus correspondientes etiquetas de salida (ej., "tráfico normal" o "ataque"). El objetivo es que el modelo infiera una función que mapee las entradas a las salidas, permitiendo clasificar nuevas instancias de datos sin etiquetas. Esta investigación se inscribe en este paradigma, buscando clasificar el tráfico de red como benigno o malicioso.

\subsubsection{Preprocesamiento de datos para modelos de Machine Learning}

La efectividad de cualquier modelo de aprendizaje automático depende críticamente de la calidad y el formato de los datos de entrada. En el contexto de la detección de intrusiones, los datos brutos de tráfico de red, capturados en formato de paquetes, no son directamente utilizables por los algoritmos de Machine Learning. El preprocesamiento de datos es, por tanto, una fase indispensable que transforma estos paquetes en un formato estructurado y significativo para el análisis.

Las etapas fundamentales del preprocesamiento en un IDS basado en ML incluyen:
\begin{itemize}
   
    \item\textbf{Extracción y Reconstrucción de Flujos de Red}: Los algoritmos de ML operan sobre "instancias" o "muestras", que en el contexto de la red, son típicamente flujos. Un flujo se define como una secuencia de paquetes que comparten una tupla común de cinco elementos: dirección IP de origen, puerto de origen, dirección IP de destino, puerto de destino y protocolo de transporte. La reconstrucción de flujos a partir de paquetes individuales permite contextualizar la información y calcular características que abarcan la duración total de la conexión.
    
     \item\textbf{Generación de Características (Feature Engineering)}: Una vez reconstruidos los flujos, se extraen o derivan diversas características numéricas y categóricas que describen el comportamiento del flujo. Estas características deben ser consistentes con las utilizadas durante la fase de entrenamiento del modelo. Ejemplos de características incluyen:
    \begin{itemize}
         \item\textbf{Métricas de Volumen}: Número total de paquetes, bytes transmitidos (bidireccional), duración del flujo.
        
         \item\textbf{Métricas de Tasa}: Paquetes por segundo, bytes por segundo.
        
         \item\textbf{Características del Protocolo}: Banderas TCP (SYN, ACK, FIN, RST, PSH, URG), tipo de protocolo (TCP, UDP, ICMP).
        
         \item\textbf{Información de Puertos}: Puertos de origen y destino.
        
         \item\textbf{Características Temporales}: Variabilidad en el tiempo entre paquetes (jitter).
        
         \item\textbf{Entropía de la Carga Útil}: Medida de la aleatoriedad en los datos de la carga útil, que puede indicar cifrado o ciertos tipos de ataques.
    \end{itemize}
    
     \item\textbf{Manejo de Variables Categóricas}: Las características no numéricas, como los nombres de protocolos (TCP, UDP, ICMP), deben transformarse a un formato numérico. La técnica común de One-Hot Encoding crea nuevas columnas binarias para cada categoría, donde un '1' indica la presencia de esa categoría y un '0' su ausencia.
    
     \item\textbf{Escalado de Características Numéricas}: Las características numéricas a menudo presentan rangos de valores muy diferentes (ej., la duración de un flujo puede ser de milisegundos a horas, mientras que el número de paquetes es un entero pequeño). El escalado (normalización o estandarización) es crucial para evitar que las características con rangos más amplios dominen desproporcionadamente en el proceso de aprendizaje del modelo. Técnicas como StandardScaler (resta la media y divide por la desviación estándar) o MinMaxScaler (escala los valores a un rango específico, como [0, 1]) son comúnmente empleadas.
    
     \item\textbf{Manejo de Valores Ausentes o Ruidosos}: Los datos de red pueden contener valores faltantes o erróneos. Es fundamental aplicar estrategias de imputación (ej., rellenar con la media, mediana o moda) o eliminación de instancias para asegurar la integridad del dataset.
    
     \item\textbf{Manejo del Desequilibrio de Clases (Class Imbalance)}: En la detección de intrusiones, el tráfico legítimo es abrumadoramente más frecuente que el tráfico malicioso (clases desequilibradas). Si no se aborda, el modelo puede sesgarse hacia la clase mayoritaria y tener un rendimiento deficiente en la detección de la clase minoritaria (ataques). Técnicas como el oversampling (ej., SMOTE para crear instancias sintéticas de la clase minoritaria), undersampling (reducir la cantidad de instancias de la clase mayoritaria), o el uso de pesos de clase durante el entrenamiento del modelo, son esenciales para mitigar este problema.

\end{itemize}
\subsubsection{Evaluación de modelos en Sistemas de Detección de Intrusiones}

La evaluación rigurosa del rendimiento de un modelo de clasificación es un paso crítico en el desarrollo de un IDS basado en aprendizaje automático. Dadas las implicaciones de seguridad, es imperativo no solo medir la precisión global, sino también comprender cómo el modelo maneja los errores, especialmente los falsos negativos, que representan ataques no detectados.

La matriz de confusión es la herramienta fundamental para una evaluación detallada. Esta tabla resume las predicciones del modelo frente a las etiquetas reales del dataset y se compone de cuatro cuadrantes clave en el contexto de clasificación binaria (Normal/Ataque):

\begin{itemize}

    \item\textbf{Verdaderos Positivos (TP)}: Instancias de tráfico malicioso correctamente clasificadas como ataques.
    
    \item\textbf{Verdaderos Negativos (TN)}: Instancias de tráfico normal correctamente clasificadas como benignas.
    
    \item\textbf{Falsos Positivos (FP)}: Instancias de tráfico normal erróneamente clasificadas como ataques (falsa alarma).
    
    \item\textbf{Falsos Negativos (FN)}: Instancias de tráfico malicioso erróneamente clasificadas como normales (ataque no detectado).

\end{itemize}

A partir de la matriz de confusión, se derivan métricas esenciales para evaluar el desempeño del IDS:

\begin{itemize}

    \item\textbf{Precisión (Accuracy)}: 
        \begin{lstlisting}[language={[LaTeX]TeX},caption={Definición de una ecuación para la Precisión},label=List.Ecuacion]
        \begin{equation}
            \left(\frac{TP + TN}{TP + TN + FP + FN}\right )
            \label{Eq.Prob1}
        \end{equation}
        \end{lstlisting}

        \begin{equation}
            \left(\frac{TP + TN}{TP + TN + FP + FN}\right )
            \label{Eq.Prob2}
        \end{equation}
    
     Mide la proporción de predicciones correctas sobre el total de predicciones. Aunque intuitiva, puede ser engañosa en datasets desequilibrados, donde una alta precisión podría deberse simplemente a la correcta clasificación de la clase mayoritaria.
    
    \item\textbf{Sensibilidad / Exhaustividad (Recall / True Positive Rate - TPR)}: 
        \begin{lstlisting}[language={[LaTeX]TeX},caption={Definición de una ecuación para la Sensibilidad},label=List.Ecuacion]
        \begin{equation}
            \left(\frac{TP}{TP + TN}\right )
            \label{Eq.Prob2}
        \end{equation}
        \end{lstlisting}

        \begin{equation}
            \left(\frac{TP}{TP + TN}\right )
            \label{Eq.Prob1}
        \end{equation}

     Cuantifica la capacidad del modelo para identificar correctamente todas las instancias positivas (ataques reales). En un IDS, maximizar el recall es a menudo una prioridad, ya que un alto número de falsos negativos implica ataques que pasan desapercibidos.
    
    \item\textbf{Especificidad (True Negative Rate - TNR)}:
        \begin{lstlisting}[language={[LaTeX]TeX},caption={Definición de una ecuación para la Especificidad},label=List.Ecuacion]
        \begin{equation}
            \left(\frac{TN}{TN + FP}\right )
            \label{Eq.Prob3}
        \end{equation}
        \end{lstlisting}

        \begin{equation}
            \left(\frac{TN}{TN + FP}\right )
            \label{Eq.Prob3}
        \end{equation}
        
     Indica la proporción de instancias negativas (tráfico normal) que son correctamente identificadas.
    
    \item\textbf{Precisión (Precision / Positive Predictive Value - PPV)}: 
        \begin{lstlisting}[language={[LaTeX]TeX},caption={Definición de una ecuación para otro tipo de precisión},label=List.Ecuacion]
        \begin{equation}
            \left(\frac{TP}{TP + FP}\right )
            \label{Eq.Prob4}
        \end{equation}
        \end{lstlisting}

        \begin{equation}
            \left(\frac{TP}{TP + FP}\right )
            \label{Eq.Prob4}
        \end{equation}

     Responde a la pregunta: de todas las instancias que el modelo clasificó como ataques, ¿cuántas fueron realmente ataques? Una alta precisión minimiza los falsos positivos, lo cual es crucial para evitar la "fatiga de alerta" en los operadores de seguridad.
    
    \item\textbf{F1-Score}:
        \begin{lstlisting}[language={[LaTeX]TeX},caption={Definición de una ecuación para la f1-score},label=List.Ecuacion]
        \begin{equation}
            \left(2*\frac{Precisión*Recall}{Precisión + Recall}\right )
            \label{Eq.Prob4}
        \end{equation}
        \end{lstlisting}

         \begin{equation}
            \left(2*\frac{Precisión*Recall}{Precisión + Recall}\right )
            \label{Eq.Prob4}
        \end{equation}
    

     Es la media armónica de la precisión y la sensibilidad. Proporciona una métrica equilibrada que es especialmente útil cuando existe un desequilibrio significativo entre las clases, ya que penaliza los modelos con altos falsos positivos y falsos negativos.
    
    \item\textbf{Tasa de Falsos Positivos (False Positive Rate - FPR)}: 
        \begin{lstlisting}[language={[LaTeX]TeX},caption={Definición de una ecuación para la tasa de falsos positivos},label=List.Ecuacion]
        \begin{equation}
            \left(\frac{FP}{FP + TN}\right )
            \label{Eq.Prob5}
        \end{equation}
        \end{lstlisting}

        \begin{equation}
            \left(\frac{FP}{FP + TN}\right )
            \label{Eq.Prob5}
        \end{equation}
    

     Complementa la especificidad y es una métrica crítica en IDS, ya que una alta tasa de falsas alarmas puede llevar a que los analistas ignoren alertas legítimas.

\end{itemize}
La Curva ROC (Receiver Operating Characteristic) y el Área bajo la Curva (AUC) son herramientas gráficas y métricas complementarias. La curva ROC representa la relación entre la Tasa de Verdaderos Positivos (TPR) y la Tasa de Falsos Positivos (FPR) a distintos umbrales de clasificación, permitiendo visualizar el compromiso entre ambas. El AUC cuantifica el rendimiento general del clasificador en todos los umbrales posibles; un valor de AUC cercano a 1.0 indica un excelente poder discriminatorio.

Para garantizar la robustez y generalizabilidad del modelo, se emplean técnicas de validación. La Validación Cruzada K-Fold es un método estándar en el que el conjunto de datos se divide en k subconjuntos (folds). El modelo se entrena k veces; en cada iteración, se utiliza un fold diferente como conjunto de prueba y los k−1 restantes como conjunto de entrenamiento. Los resultados se promedian para obtener una estimación más fiable del rendimiento del modelo, reduciendo el sesgo y la varianza que podrían surgir de una única división aleatoria. Además, es esencial distinguir entre el conjunto de entrenamiento, el conjunto de validación (utilizado para el ajuste de hiperparámetros) y el conjunto de prueba (utilizado únicamente para la evaluación final imparcial del modelo).

\subsection{Ciencia de datos}\label{Sec.Referencias}

La Ciencia de Datos emerge como una disciplina transversal que integra metodologías y principios de campos como la estadística, la informática, las matemáticas, el conocimiento del dominio y la visualización, con el propósito primordial de extraer conocimiento, patrones e insights accionables a partir de volúmenes de datos complejos y heterogéneos. Su objetivo no es meramente el procesamiento de información, sino la capacidad de transformar datos brutos en inteligencia estratégica, facilitando la toma de decisiones informadas y la predicción de eventos futuros. En la actualidad, su aplicación es ubicua, abarcando desde la optimización de procesos empresariales hasta la investigación científica y, pertinentemente para este proyecto, la ciberseguridad.

Dentro del vasto espectro de la Ciencia de Datos, el aprendizaje automático (Machine Learning - ML) y el aprendizaje profundo (Deep Learning - DL) constituyen dos de sus pilares más potentes y de mayor crecimiento. Mientras que el aprendizaje automático engloba un conjunto de algoritmos que permiten a los sistemas aprender de los datos para realizar predicciones o tomar decisiones sin ser programados explícitamente para cada tarea, el aprendizaje profundo representa una subcategoría del ML que se basa en redes neuronales artificiales con múltiples capas, capaces de aprender representaciones de datos jerárquicas y abstractas.

En el marco de este proyecto de fin de grado, la Ciencia de Datos se erige como el eje metodológico para abordar el problema de la detección de intrusiones en redes. La fase inicial implica la adquisición y preprocesamiento de grandes volúmenes de datos de tráfico de red, los cuales se presentan con una multitud de características y métricas de conexión. Este proceso es crucial para transformar el tráfico en bruto en un formato estructurado y apto para el análisis computacional.

Posteriormente, y de manera exclusiva, se recurrirá a las técnicas de Machine Learning para desarrollar la capacidad predictiva del sistema. Específicamente, se ha optado por el entrenamiento de un modelo basado en el algoritmo Random Forest. Este modelo, conocido por su robustez, eficiencia y capacidad para manejar conjuntos de datos de alta dimensionalidad y características complejas, será el encargado de resolver un problema de clasificación. Su función cardinal será la de discernir, con alta fiabilidad, entre patrones de tráfico de red que corresponden a un comportamiento normal (benigno) y aquellos que denotan una actividad maliciosa o una posible intrusión. La implementación de esta aproximación permitirá que el sistema de detección evolucione y se adapte a nuevas amenazas basándose en el aprendizaje continuo de los patrones inherentes a los datos de la red.

\section{Descripción del proyecto}
El propósito principal de este proyecto es la implementación de un capturador de paquetes en tiempo real que sea capaz de capturar dichos paquetes y agruparlos en distintos flujos para que posteriormente, se muestren en una aplicación web basada en un sistema de detección de intrusiones aplicándole mecanismos de machine learning.

El proceso comienza con la captura del tráfico de red mediante una aplicación que tiene como fin la captura de paquetes en claro y a partir de estos, organiza los paquetes en distintos flujos y extrae las características necesarias para que pasen a un formato parecido al que se rige CICFlowMeter, que es una herramienta de generación y análisis de flujos de tráfico de red que produce flujos bidireccionales a partir de paquetes de red.

Por último, la aplicación mostrará en un dashboard, los flujos de paquetes en un panel, y en otro, aparecerán cuales de estos se consideran una amenaza. La lógica de control para discernir entre un flujo benigno y maligno, se encargá el modelo de IA que hemos elegido, el cual, estará integrado en el backend al igual que el propio capturador en tiempo real que hemos implementado.

El presente Proyecto de Fin de Grado aborda el diseño e implementación de un Sistema de Detección de Intrusiones (IDS) innovador y multifuncional que integra la captura de tráfico de red en tiempo real con técnicas avanzadas de aprendizaje automático (Machine Learning). El objetivo primordial es desarrollar una solución robusta capaz de discernir, con alta fiabilidad, entre tráfico de red legítimo (benigno) y actividad maliciosa o anómala.

El objetivo principal de este trabajo es la creación de una herramienta especializada para la captura y el procesamiento de paquetes de red. Esta aplicación ha sido diseñada con la capacidad de operar en tiempo real, monitorizando continuamente el flujo de datos a través de la red. Un aspecto distintivo de esta herramienta es su versatilidad para generar y persistir flujos de paquetes, transformando la información de bajo nivel de los paquetes individuales en un formato estructurado y significativo para el análisis posterior. La aplicación permite la conversión de estos flujos en formatos de salida estandarizados como .csv o .txt, lo que facilita la creación de nuevas bases de datos de tráfico de red. Esta funcionalidad es crucial, ya que emula y complementa la metodología empleada en datasets de referencia en ciberseguridad, como los de la serie CIC-IDS (ej., CIC-IDS2017, CIC-IDS2018, CIC-IDS2019), proporcionando una base para la investigación y el desarrollo futuros en el campo. Además, la capacidad de invocación por consola dota a la herramienta de flexibilidad para ser integrada en diversos entornos y automatizaciones, permitiendo la generación de datos para el entrenamiento y la evaluación de modelos de Machine Learning.

Para validar y demostrar la eficacia del capturador de paquetes y para completar la arquitectura del IDS, se ha implementado un módulo de detección basado en Machine Learning. Este módulo utiliza un modelo de clasificación Random Forest, seleccionado por su reconocida capacidad para manejar grandes volúmenes de datos con alta dimensionalidad y su robustez ante el ruido. El modelo ha sido entrenado para identificar patrones y características específicas que distinguen el tráfico normal de diversas categorías de ataques, lo que le permite clasificar de forma predictiva cada flujo de red entrante.

Complementando estas funcionalidades, se ha desarrollado una aplicación web, concebida como un dashboard intuitivo, que permite visualizar en tiempo real la actividad del capturador de paquetes y el rendimiento del modelo de detección. Este interfaz gráfico no solo ofrece una representación clara de los eventos de red, sino que también sirve como una herramienta práctica para monitorizar las detecciones de tráfico malicioso, facilitando la interacción del usuario con el IDS.

\section{Objetivos y motivación}

La proliferación incesante de las tecnologías digitales y la interdependencia sistémica de las infraestructuras de red han configurado un panorama donde la ciberseguridad ya no es una mera consideración técnica, sino un pilar fundamental para la operatividad y la resiliencia de cualquier entidad, desde organizaciones gubernamentales hasta empresas privadas y usuarios individuales. En este entorno, la capacidad de detectar y responder eficazmente a las intrusiones cibernéticas se ha vuelto crítica. Las amenazas evolucionan con una celeridad asombrosa, adoptando formas cada vez más sofisticadas y evasivas que superan las capacidades de los sistemas de defensa convencionales, como los Sistemas de Detección de Intrusiones (IDS) basados exclusivamente en firmas. Estos últimos, si bien eficientes contra amenazas conocidas y previamente catalogadas, son inherentemente limitados frente a los ataques "zero-day" o las variantes polimórficas que modifican sus patrones para eludir la detección.

Esta brecha en las capacidades de detección tradicional ha impulsado la necesidad de explorar paradigmas más adaptativos y predictivos. El aprendizaje automático (Machine Learning - ML) emerge como una solución prometedora, dada su inherente capacidad para identificar patrones complejos, anomalías y comportamientos desviados en grandes volúmenes de datos, sin requerir una programación explícita para cada posible amenaza \cite{Zhang2022AICybersecurity}. La aplicación de ML en la detección de intrusiones no solo promete una mayor tasa de detección de ataques novedosos, sino que también ofrece la posibilidad de reducir la dependencia de las actualizaciones manuales de firmas y reglas.

La motivación principal de este Proyecto de Fin de Grado radica precisamente en la respuesta a esta necesidad crítica. Se aspira a contribuir de forma tangible al campo de la ciberseguridad mediante el desarrollo de un IDS que fusione la monitorización en tiempo real del tráfico de red con la inteligencia predictiva del Machine Learning. Más allá de la mera detección, se reconoce un desafío recurrente en la investigación de IDS: la escasez de conjuntos de datos de tráfico de red etiquetados que sean representativos y actuales para el entrenamiento y la validación de modelos de Machine Learning \cite{PolaniaArias2021EvaluacionMLIDS}. La creación de tales datasets es un proceso laborioso y costoso, lo que limita el progreso en la evaluación comparativa y el desarrollo de nuevas arquitecturas de detección. Esta dificultad intrínseca proporciona una motivación adicional y un objetivo secundario crucial para este proyecto: desarrollar una herramienta que no solo detecte intrusiones, sino que también facilite la generación de datasets de tráfico de red de alta calidad, emulando las metodologías de benchmarks reconocidos como los de la serie CIC-IDS (ej., CIC-IDS2017, CIC-IDS2018, CIC-IDS2019). Esta capacidad es fundamental para impulsar futuras investigaciones y para el entrenamiento continuo de modelos más robustos y adaptativos.

Con base en esta profunda motivación y el análisis de las limitaciones existentes, los objetivos de este proyecto se estructuran de la siguiente manera:

En primer lugar, el objetivo primordial es el diseño, desarrollo e implementación de un Sistema de Detección de Intrusiones (IDS) holístico y funcional, anclado en los principios del aprendizaje automático. Este sistema debe ser capaz de analizar el tráfico de red en tiempo real, distinguiendo con precisión entre patrones de comportamiento benignos y maliciosos, para así elevar la postura de seguridad de la infraestructura monitoreada.

Para materializar este objetivo general, se han delineado varios objetivos específicos que guiarán las fases de desarrollo:

\begin{itemize}

    \item\textbf{Concebir y Construir una Herramienta Avanzada para la Captura y el Preprocesamiento de Tráfico de Red}: El desarrollo de una aplicación personalizada es fundamental. Esta herramienta no solo deberá ser eficiente en la captura de paquetes en tiempo real, sino que también integrará capacidades robustas para la reconstrucción de flujos de conexión y la extracción detallada de un amplio conjunto de características pertinentes para el análisis de seguridad. Un valor añadido de esta herramienta será su funcionalidad para generar salidas estructuradas en formatos estándar como .csv o .txt, lo que permitirá la creación sistemática de nuevas bases de datos de tráfico de red. Esta capacidad de generación de datasets, invocable incluso desde la consola, es vital para la replicabilidad, la extensibilidad y la contribución a la comunidad de investigación en ciberseguridad.
    
    \item\textbf{Investigar, Implementar y Optimizar un Modelo de Clasificación de Machine Learning de Alto Rendimiento}: La selección de un algoritmo de aprendizaje automático es crucial. Se ha optado por el algoritmo Random Forest debido a su probada eficacia en problemas de clasificación multiclase y binaria, su resistencia al sobreajuste, y su capacidad para gestionar conjuntos de datos con un elevado número de características y una significativa dimensionalidad \cite{Breiman2001RandomForests}. El modelo será sometido a un riguroso proceso de entrenamiento con datasets de tráfico etiquetado, con el fin de optimizar su rendimiento para la minimización simultánea de falsos positivos (que generan fatiga en los analistas) y, lo que es más crítico, de falsos negativos (que implican ataques no detectados) \cite{PolaniaArias2021EvaluacionMLIDS}.
    
    \item\textbf{Asegurar la Integración Fluida entre la Captura de Datos y el Motor de Detección de Machine Learning}: Se establecerá un pipeline de procesamiento de datos que permita la alimentación continua y eficiente de los flujos de red preprocesados desde la herramienta de captura hacia el modelo de Machine Learning. Esta integración garantizará que el IDS pueda clasificar el tráfico de forma dinámica y emitir alertas inmediatas ante la detección de actividades sospechosas, operando en un modo casi en tiempo real.
    
    \item\textbf{Diseñar y Desarrollar una Interfaz Web Intuitiva para la Monitorización y Visualización}: Para facilitar la interacción del usuario y la supervisión del sistema, se implementará una aplicación web que actúe como un dashboard. Esta interfaz ofrecerá una representación clara y amigable de la actividad de la red, los eventos de detección generados por el modelo de Machine Learning, y las métricas operativas del IDS. La visualización en tiempo real es clave para proporcionar a los administradores de seguridad una panorámica inmediata del estado de la red.
    
    \item\textbf{Realizar una Evaluación Exhaustiva y Rigurosa del Rendimiento del IDS Propuesto}: La validación empírica del sistema es indispensable. Se llevarán a cabo pruebas exhaustivas utilizando metodologías de evaluación estándar para problemas de clasificación en el ámbito de la ciberseguridad. Esto incluirá el cálculo y análisis de métricas como la precisión (accuracy), sensibilidad (recall), precisión (precision), F1-score, la construcción y análisis de la matriz de confusión, y la curva ROC con su respectivo AUC \cite{PolaniaArias2021EvaluacionMLIDS}. El objetivo es demostrar la eficacia del IDS en la identificación de diferentes tipos de ataques y validar su viabilidad como una herramienta de seguridad operativa y de valor añadido.
 
\end{itemize}

A través de la consecución de estos objetivos, este proyecto no solo aspira a generar una solución técnica funcional para la detección de intrusiones, sino también a contribuir al cuerpo de conocimiento en la intersección de la ciberseguridad y la inteligencia artificial, ofreciendo herramientas y metodologías que pueden ser de utilidad para la comunidad científica y profesional.

\section{Metodología de desarrollo de software}
Una metodología de desarrollo de software es un conjunto de prácticas,
técnicas y procedimientos que se utilizan para organizar, planificar y ejecutar proyectos de desarrollo de software. Su objetivo principal es mejorar la eficiencia y la calidad del proceso de desarrollo, asegurando que el software se entregue a tiempo, dentro del presupuesto y cumpla con los requisitos del cliente. Estas metodologías proporcionan un marco estructurado que guía a los equipos de desarrollo a través de las diferentes fases del ciclo de vida del software, desde la concepción y el diseño hasta la implementación, prueba y mantenimiento.

\subsection{SCRUM como modelo de desarrollo}
Dada la estructura del proyecto y sus dimensiones, se ha optado por utilizar la metodología Scrum. Scrum es una metodología ágil que se basa en la realización de incrementos pequeños y manejables, permitiendo así una mayor flexibilidad y adaptación a los cambios a lo largo del proceso de desarrollo.

Algunos de los componentes clave de Scrum son:
\begin{itemize}

    \item\textbf{Roles: Scrum define tres roles principales}:
    \begin{itemize}
 
        \item\textbf{Product Owner}: Responsable de maximizar el valor del producto y gestionar el backlog del producto.
        \item\textbf{Scrum Master}: Facilita el proceso Scrum, ayuda al equipo a seguir las prácticas de Scrum y elimina impedimentos.
        \item\textbf{Equipo de Desarrollo}: Grupo multifuncional que trabaja en la entrega de incrementos del producto.
          
    \end{itemize}
    \item\textbf{Eventos}:
    \begin{itemize}

        \item\textbf{Sprint}: Periodo de tiempo fijo (generalmente de 1 a 4 semanas) durante el cual se realiza un incremento del producto.
        \item\textbf{Sprint Planning}: Reunión para planificar el trabajo que se realizará durante el sprint.
        \item\textbf{Daily Scrum}: Reunión diaria de 15 minutos para sincronizar el trabajo del equipo.
        \item\textbf{Sprint Review}: Reunión para revisar el incremento y adaptar el backlog del producto si es necesario.
        \item\textbf{Sprint Retrospective}: Reunión para reflexionar sobre el sprint y buscar mejoras continuas.
           
    \end{itemize}
    \item\textbf{Artefactos}:
    \begin{itemize}

        \item\textbf{Product Backlog}: Lista priorizada de todo el trabajo que se necesita en el producto.
        \item\textbf{Sprint Backlog}: Lista de tareas seleccionadas del product backlog para completarse en el sprint actual.
        \item\textbf{Increment}: El resultado de un sprint, que debe ser un producto utilizable y potencialmente desplegable.
          
    \end{itemize}
   
\end{itemize}

Scrum se caracteriza por su enfoque en la transparencia, inspección y
adaptación, permitiendo a los equipos responder rápidamente a los cambios y mejorar continuamente.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.8\textwidth]{imagenes/proceso_scrum.png}
  \caption{Proceso de las fases de desarrollo SCRUM.}
  \label{Fig.FasesSCRUM}
\end{figure}

\subsection{Adaptación de SCRUM para proyectos individuales}
Aunque Scrum está diseñado originalmente para equipos, sus principios y
estructura pueden adaptarse para el desarrollo por una sola persona. Basándonos en el artículo “Scrum for One” de Lucidchart [5] vamos a desarrollar el proceso de adaptación de esta metodología para una sola persona. 

En un entorno de un solo desarrollador, la misma persona asume los roles de Product Owner, Scrum Master y Equipo de Desarrollo. Esto requiere una clara organización y autogestión. El desarrollador único debe gestionar el backlog del producto, facilitar su propio proceso de desarrollo y eliminar impedimentos de manera autónoma. Este enfoque puede ser beneficioso ya que el desarrollador tiene control total sobre las decisiones y priorizaciones, lo que puede agilizar el proceso. 

Se pueden mantener los eventos clave de Scrum, pero de manera simplificada y flexible para una sola persona:
\begin{itemize}
    
    \item\textbf{Sprint Planning}: Planificar los sprints sigue siendo esencial. El desarrollador único debe dedicar tiempo a establecer objetivos claros para cada sprint, definir las tareas necesarias y priorizarlas en el sprint backlog.
    
    \item\textbf{Daily Scrum}: Aunque no hay un equipo con el cual sincronizarse, el desarrollador puede realizar una auto-revisión diaria. Esto ayuda a mantener el enfoque y la disciplina, permitiendo reflexionar sobre el progreso y ajustar el plan si es necesario.
    
    \item\textbf{Sprint Review}: Al final de cada sprint, el desarrollador revisa el trabajo completado. Este evento puede incluir la evaluación de cómo se han alcanzado los objetivos del sprint y la identificación de cualquier ajuste necesario en el backlog del producto.

    \item\textbf{Sprint Retrospective}: Es crucial para la mejora continua. El desarrollador reflexiona sobre lo que funcionó bien y lo que se puede mejorar para los próximos sprints.
  
\end{itemize}

Los artefactos de Scrum también siguen siendo importantes ya que una vez
vistos las modificaciones en los eventos se puede concluir que los artefactos se usarán de la misma forma que en un equipo con varias personas. 

Además, la naturaleza iterativa de Scrum permite una gran flexibilidad y
capacidad de adaptación. Esto significa que el desarrollador puede responder rápidamente a los cambios y ajustar su enfoque según sea necesario, lo cual es vital en un entorno de desarrollo dinámico. Las retrospectivas regulares fomentan una cultura de mejora continua, permitiendo identificar áreas de mejora y aplicar cambios en ciclos cortos, lo que facilita la evolución y optimización del proceso de desarrollo.

\subsection{Sprints de desarrollo}\label{Sec.Sprints}
El desarrollo de un sistema tan complejo como un Sistema de Detección de Intrusiones (IDS) que integra captura de tráfico en tiempo real, procesamiento de datos, análisis mediante aprendizaje automático y una interfaz web, exige una metodología ágil que permita gestionar la complejidad, adaptarse a los desafíos emergentes y entregar valor de forma incremental. Para este proyecto, se adoptó un enfoque basado en Sprints de Desarrollo, inspirados en la metodología SCRUM. Esta aproximación facilitó la división del proyecto en iteraciones cortas y manejables, cada una enfocada en la consecución de funcionalidades específicas, garantizando así un progreso continuo y la capacidad de integrar retroalimentación a lo largo del ciclo de vida del desarrollo. A continuación, se describen los sprints planificados y ejecutados, delineando los objetivos y entregables clave de cada fase.


\textbf{Sprint 1: Fundamentos Teóricos y Análisis de Requisitos}

Este sprint inaugural se centró en la construcción de una base de conocimiento sólida indispensable para abordar un proyecto de la envergadura de un IDS basado en Machine Learning. Las primeras semanas se dedicaron a una inmersión profunda en la literatura científica y técnica. Se revisó una colección de papers y artículos especializados proporcionados por la dirección del proyecto, enfocados en la intersección de la inteligencia artificial y la ciberseguridad, con especial énfasis en los principios y arquitecturas de los Sistemas de Detección de Intrusiones \cite{Charmet2022XAI}, \cite{Zhang2022AICybersecurity}. Paralelamente, se llevó a cabo una investigación exhaustiva en bases de datos académicas y repositorios en línea para complementar el entendimiento de conceptos fundamentales de IDS y los distintos paradigmas de inteligencia artificial, inicialmente explorando tanto el aprendizaje supervisado como el no supervisado. Una vez afianzado el conocimiento sobre las técnicas de aprendizaje automático, la atención se dirigió hacia la identificación y análisis de conjuntos de datos (datasets) adecuados para el entrenamiento y evaluación de modelos de IDS. Se familiarizó con la estructura, características y taxonomía de datasets de referencia en la comunidad, tales como CIC-IDS (2017, 2018, 2019), KDD99, NSL-KDD, y UNSW \cite{PolaniaArias2021EvaluacionMLIDS}, comprendiendo sus particularidades y la información que estos proporcionan para la detección de anomalías. Este sprint concluyó con una clara comprensión del marco teórico y una dirección definida hacia la aplicación de técnicas de aprendizaje supervisado, sentando las bases conceptuales para las fases de implementación.

\textbf{Sprint 2: Diseño del Módulo de Captura y Selección de Tecnologías} 

Con la base teórica establecida, el segundo sprint se focalizó en el diseño arquitectónico del módulo de captura de paquetes y la selección de las tecnologías más adecuadas. La fase inicial de diseño incluyó la evaluación de posibles integraciones con herramientas existentes como Wireshark; sin embargo, las complejidades asociadas a su código base y las potenciales dificultades de integración con componentes de IA llevaron a su descarte. Se llevó a cabo una investigación profunda sobre los lenguajes de programación y las librerías disponibles para el desarrollo de capturadores de paquetes eficientes. Esta fase concluyó con la elección de Python, un lenguaje que demostró ser idóneo por su robustez, su amplio ecosistema de librerías para la manipulación de redes \cite{Shrefler2017Networking} y su facilidad de integración con frameworks de Machine Learning \cite{Pedregosa2011ScikitLearn}. Crucialmente, se identificó una API especializada que proporcionaba un conjunto rico de funcionalidades para la extracción de características detalladas a partir del tráfico de red. Al finalizar este sprint, se dispuso de un diseño conceptual claro del capturador y una pila tecnológica definida para su implementación.

\textbf{Sprint 3: Implementación Básica del Capturador de Paquetes y CLI}

Este sprint se dedicó a la implementación inicial del capturador de paquetes en Python. Se desarrolló la funcionalidad core que permite la intercepción y procesamiento de paquetes de red. El foco principal fue la creación de una interfaz de línea de comandos (CLI) que posibilitara la invocación del programa con parámetros específicos, tales como la interfaz de red a monitorear. Se logró que el capturador pudiera iniciar y detener la recolección de flujos de paquetes desde la terminal, sentando las bases operativas de la herramienta. Las pruebas iniciales se centraron en verificar la correcta captura y el parseo básico de los datos de los paquetes, asegurando que el flujo de información se manejara de manera estable.

\textbf{Sprint 4: Desarrollo de la Funcionalidad de Exportación de Datos}

Continuando con el módulo de captura, el cuarto sprint se concentró en una funcionalidad crítica para la utilidad del proyecto: la exportación de los flujos de paquetes procesados a formatos estructurados. Reconociendo la dificultad de obtener datasets de IDS actualizados, se implementó la capacidad de generar archivos en formato .csv y .txt a partir de los flujos capturados. Esta funcionalidad es fundamental, ya que permite que el IDS contribuya a la creación de nuevas bases de datos de tráfico de red, emulando la estructura de datasets de referencia como los de la serie CIC-IDS. Esta capacidad es clave para la investigación y el desarrollo de modelos de Machine Learning, al proporcionar acceso a datos más representativos del tráfico moderno. Se realizaron pruebas exhaustivas para validar la integridad y el formato de los archivos generados, asegurando que pudieran ser utilizados directamente para el preprocesamiento y entrenamiento de modelos de IA.

\textbf{Sprint 5: Definición de la Arquitectura Web y Prototipado de la Interfaz}

Con el módulo de captura en progreso, este sprint abordó el diseño y la arquitectura de la aplicación web que albergaría el IDS completo. Se llevó a cabo una investigación de frameworks de desarrollo web full-stack, evaluando opciones como Angular, React (para frontend) combinados con backends como Laravel (PHP) o Flask/Django (Python). La decisión final se inclinó por Reflex, un framework de Python que ofrecía una solución unificada para el desarrollo full-stack en un solo lenguaje, lo que simplificaba la integración con los componentes de Python existentes y optimizaba el flujo de trabajo. Una vez seleccionado el framework, se procedió al análisis detallado y el diseño de la interfaz de usuario (UI). Esto incluyó la elaboración de diagramas de flujo de usuario, la creación de mockups visuales y wireframes para definir la estructura y navegación del dashboard. El objetivo fue crear un diseño intuitivo que facilitara la visualización del tráfico de red y las futuras detecciones.

\textbf{Sprint 6: Implementación del Frontend y Conexión al Backend del Capturador}

Este sprint se centró en la construcción del frontend de la aplicación web según los diseños establecidos en el sprint anterior. Se implementaron las principales pantallas y componentes de la interfaz de usuario, dando vida al dashboard. Simultáneamente, se inició el desarrollo de la capa de backend de la aplicación web, preparada para recibir y procesar los datos del capturador. Un hito crítico de este sprint fue la integración inicial del módulo de captura de paquetes con el backend de la aplicación web. Para lograr una comunicación en tiempo real y asíncrona entre ambos componentes, se implementó un sistema basado en colas síncronas. Las pruebas de este sprint se dirigieron a verificar que los datos de tráfico capturados por el módulo de Python se transmitieran correctamente al backend y que el panel de visualización del tráfico de red en el frontend se actualizara en tiempo real, confirmando la operatividad del flujo de datos de extremo a extremo.

\textbf{Sprint 7: Preprocesamiento y Unificación del Dataset para ML}

Paralelo al avance de la aplicación web, este sprint se dedicó intensivamente a la preparación del conjunto de datos para el entrenamiento del modelo de Machine Learning. Se seleccionó el CIC-IDS2018 como dataset principal debido a su representatividad y variedad de ataques \cite{PolaniaArias2021EvaluacionMLIDS}. La tarea crucial fue la unificación de los diversos ficheros que componían este dataset (correspondientes a diferentes días de captura y tipos de ataque) en una única base de datos cohesiva. Esta consolidación fue fundamental para asegurar que el modelo de aprendizaje tuviera acceso a un volumen variado y robusto de flujos de tráfico, crucial para su capacidad de generalización \cite{PolaniaArias2021EvaluacionMLIDS}. Tras la unificación, se llevó a cabo un exhaustivo proceso de preprocesamiento, análisis y limpieza de los datos. Esto incluyó la gestión de valores ausentes, la corrección de inconsistencias, la normalización/estandarización de características numéricas y la codificación de variables categóricas, preparando el dataset para la fase de entrenamiento \cite{James2013ISLR}.

\textbf{Sprint 8: Entrenamiento y Evaluación del Modelo de Machine Learning (Random Forest)}

Una vez que el dataset estuvo preparado y limpio, este sprint se centró por completo en el desarrollo y la evaluación del modelo de Machine Learning. Se procedió al entrenamiento del clasificador Random Forest, un algoritmo elegido por su eficacia en la detección de intrusiones en entornos de alta dimensionalidad y su robustez ante el sobreajuste \cite{Breiman2001RandomForests}. Durante esta fase, se ajustaron los hiperparámetros del modelo para optimizar su rendimiento. Tras el entrenamiento, se realizó una evaluación rigurosa del modelo utilizando métricas clave de clasificación como la precisión (accuracy), la sensibilidad (recall), la precisión (precision), el F1-score, y el análisis detallado de la matriz de confusión y la curva ROC con su Área bajo la Curva (AUC) \cite{PolaniaArias2021EvaluacionMLIDS}. Este análisis permitió determinar la capacidad predictiva del modelo y su fiabilidad para distinguir entre tráfico benigno y malicioso. El entregable de este sprint fue un modelo de Machine Learning entrenado y validado, listo para ser integrado en el sistema en tiempo real.

\textbf{Sprint 9: Integración Final del Modelo ML y Detección en Tiempo Real}

Este sprint fue el punto culminante de la integración de todos los componentes del IDS. El modelo de Machine Learning entrenado fue integrado en el backend de la aplicación web. Esto permitió que los flujos de paquetes, capturados por el módulo Python y transmitidos a través de las colas síncronas al backend, fueran ahora analizados en tiempo real por el modelo Random Forest. La funcionalidad clave de este sprint fue la capacidad del sistema para determinar si un flujo de tráfico era benigno o malicioso de forma automática y mostrar esta clasificación instantáneamente en la interfaz web. Se realizaron pruebas de integración de extremo a extremo para asegurar que el ciclo completo, desde la captura hasta la predicción y la visualización, operara sin fallos y con la menor latencia posible.

\textbf{Sprint 10: Pruebas Exhaustivas, Optimización y Validación Final}

El sprint final se dedicó a la consolidación y refinamiento de todo el sistema. Se ejecutaron multitud de pruebas exhaustivas y multifacéticas para garantizar la robustez, la estabilidad y el rendimiento óptimo de la aplicación en su conjunto. Esto incluyó pruebas de estrés para verificar la capacidad del capturador y del modelo bajo cargas de tráfico elevadas, pruebas de usabilidad de la interfaz web, y pruebas de regresión para asegurar que las nuevas funcionalidades no introdujeran fallos en componentes existentes. Asimismo, se llevó a cabo una fase de optimización general del rendimiento de la aplicación, buscando mejorar los tiempos de respuesta del capturador, la latencia en la predicción del modelo y la fluidez de la interfaz de usuario. Finalmente, se realizó una validación completa del sistema \cite{NIST2020SP800-115} para confirmar que todos los objetivos del proyecto habían sido alcanzados, que el IDS operaba de manera confiable y que estaba listo para su defensa.

\section{Prespuesto del proyecto}

La planificación y ejecución de cualquier proyecto, incluyendo un Trabajo Fin de Grado, conlleva una serie de costes asociados que deben ser estimados y desglosados para comprender la inversión económica necesaria. Este apartado detalla el presupuesto aproximado del desarrollo del Sistema de Detección de Intrusiones (IDS) y la herramienta de generación de datasets a lo largo de los ocho meses de duración estimados para el proyecto. Para la elaboración de este presupuesto, se han considerado los recursos humanos implicados, los recursos materiales utilizados y los costes indirectos asociados.

\textbf{Recursos Humanos}

Aunque este proyecto ha sido realizado por un único estudiante en el marco de su Trabajo Fin de Grado, para fines de presupuesto y para reflejar un escenario de desarrollo profesional, se estimará el coste equivalente a la contratación de un Programador Junior / Investigador Junior en Ciberseguridad. Este perfil asumiría las responsabilidades de diseño, programación, investigación y gestión del proyecto.

Para la estimación salarial, se puede tomar como referencia el Convenio colectivo estatal de empresas de consultoría y estudios de mercado y de la opinión pública, o bien, promedios de salarios para perfiles junior en el sector tecnológico en España. Considerando un salario anual bruto para un programador/investigador junior en un rango de 18.000€ a 22.000€ (más realista para el perfil y las habilidades requeridas en Machine Learning y ciberseguridad que los 14.800,66€ mencionados en un convenio específico que puede no aplicar directamente al rol técnico avanzado), tomaremos un valor medio de 20.000€ anuales para este cálculo.
\begin{itemize}

    \item\textbf{Salario base anual estimado}: 20.000,00 \euro{}
    
    \item \textbf{Salario mensual:}$\displaystyle \frac{20.000,00}{12} = 1.666,67 $ \euro{}/mes
   
    \item\textbf{Coste total de recursos humanos (8 meses)}:$\displaystyle 1.666,67 \text{ \euro{}/mes} \times 8 \text{ meses} = 13.333,36 $ \euro{}

\end{itemize}

A este coste salarial se le deberían añadir los costes sociales (aportaciones a la Seguridad Social por parte de la empresa), que en España suelen rondar el 30-35\% del salario bruto. Para una estimación, consideraremos un 32\%.
\begin{itemize}
    
    \item\textbf{Costes Sociales mensuales}:$\displaystyle 1.666,67 \text{ \euro{}/mes} \times 0.32 = 533,33 $ \euro{}/mes
    
    \item\textbf{Costes Sociales totales (8 meses)}:$\displaystyle 533,33 \text{ \euro{}/mes} \times 8 \text{ meses} = 4.266,64 $ \euro{}
    
    \item\textbf{Coste total de Recursos Humanos (Salario + Costes Sociales)}:$\displaystyle 13.333,36 \text{ \euro{}} + 4.266,64 \text{ \euro{}} = 17.600,00 $ \euro{}
    
\end{itemize}

\textbf{Recursos Materiales}
 
El desarrollo del proyecto ha requerido el uso de equipamiento informático para la programación, la ejecución de modelos de Machine Learning y la realización de pruebas de captura de tráfico en tiempo real. Aunque los equipos ya eran propiedad del desarrollador, para el presupuesto se considerará una amortización proporcional al tiempo de uso del proyecto.

Se han utilizado los siguientes equipos principales:

\begin{itemize}

    \item\textbf{Ordenador Portátil (ej. HP Pavilion 16-a0003N o similar)}:
        \begin{itemize}

            \item\textbf{Coste de adquisición}: 899,57 \euro{} 
            
            \item\textbf{Vida útil estimada}: 4 años (48 meses)
            
            \item\textbf{Amortización mensua}l:$\displaystyle \frac{899,57}{48} = 18,74 $ \euro{}/mes
            
            \item\textbf{Coste por 8 meses}:$\displaystyle 18,74 \text{ \euro{}/mes} \times 8 \text{ meses} = 149,92 $ \euro{}
                   
        \end{itemize}
    
    \item\textbf{Smartphone (ej. Xiaomi Mi 10 5G o similar, para pruebas de red móvil si aplicase, o como dispositivo de test para la app web)}:
    
        \begin{itemize}
        
            \item\textbf{Coste de adquisición}: 300,00 \euro{} (basado en la referencia del compañero)
            
            \item\textbf{Vida útil estimada}: 3 años (36 meses)
            
            \item\textbf{Amortización mensual}:$\displaystyle \frac{300,00}{36} = 8,33 $ \euro{}/mes
            
            \item\textbf{Coste por 8 meses}:$\displaystyle 8,33 \text{ \euro{}/mes} \times 8 \text{ meses} = 66,64 $ \euro{}
                
        \end{itemize}
    
    \item\textbf{Software y Herramientas (Licencias/Suscripciones)}:
    Aunque la mayoría de las herramientas utilizadas (Python, librerías como Scikit-learn, Reflex, Wireshark/Scapy, etc.) son de código abierto o gratuitas, en un entorno profesional se considerarían costes para herramientas de desarrollo (IDEs avanzados), sistemas de control de versiones premium (GitHub Enterprise), o servicios de computación en la nube para el entrenamiento de modelos más grandes (ej. Google Colab Pro, AWS, Azure, etc.). Para este TFG, asumiremos costes mínimos o nulos por licencias de software, pero se podría estimar un coste simbólico por el acceso a ciertos recursos o licencias de herramientas de diseño/gestión si se hubieran utilizado versiones de pago. Aquí asumiremos 0€ dado el contexto de TFG, pero se podría incluir un pequeño monto (ej. 50-100€) para una licencia de IDE o software de diseño.
    
    \item\textbf{Conexión a Internet y Electricidad}:
    Estos costes se suelen incluir en los costes operativos indirectos de un proyecto. Considerando una parte proporcional del coste de una conexión doméstica y el consumo eléctrico del equipo durante las horas de desarrollo.
    
        \begin{itemize}
        
            \item\textbf{Conexión a Internet}:$\displaystyle \frac{50 \text{ \euro{}/mes}}{2} \text{ (uso profesional)} \times 8 \text{ meses} = 200,00 $ \euro{}
            
            \item\textbf{Electricidad}:$\displaystyle \frac{30 \text{ \euro{}/mes}}{2} \text{ (uso profesional)} \times 8 \text{ meses} = 120,00 $ \euro{}
                    
        \end{itemize}    
    
    \item\textbf{Total de Recursos Materiales}:$\displaystyle 149,92 \text{ \euro{}} + 66,64 \text{ \euro{}} + 0,00 \text{ \euro{}} + 200,00 \text{ \euro{}} + 120,00 \text{ \euro{}} = 536,56 $ \euro{}
   
\end{itemize}

\textbf{Costes Indirectos y Contingencias} 

Estos costes cubren gastos generales y posibles imprevistos no directamente asignables a las categorías anteriores. Incluyen, por ejemplo, material de oficina, formación específica no prevista, gastos de comunicación, etc. Se suele estimar un porcentaje sobre el total de los costes directos.
\begin{itemize}
    \item\textbf{Costes Directos Totales (Recursos Humanos + Recursos Materiales)}: $\displaystyle 17.600,00 \text{ \euro{}} + 536,56 \text{ \euro{}} = 18.136,56 \text{ \euro{}}$
    
    \item\textbf{Contingencias (10\% de los costes directos)}:$\displaystyle 18.136,56 \text{ \euro{}} \times 0.10 = 1.813,66 \text{ \euro{}}$
\end{itemize}

\textbf{Resumen del presupuesto final del proyecto}

En la siguiente tabla ~\ref{Tabla.Presupuesto} se puede observar el resumen del desglose de los costes estimados para el desarrollo de este Trabajo Fin de Grado durante un periodo de 8 meses:

\input{tablas/tablaPresupuesto}

Este presupuesto final de aproximadamente 19.910,22 euros proporciona una estimación realista del valor de mercado de un proyecto de esta envergadura y complejidad si se llevara a cabo en un entorno profesional, abarcando la investigación, el desarrollo de un módulo de captura especializado, la implementación de un modelo de Machine Learning y la creación de una aplicación web interactiva.

\section{Planificación de tiempo y costes}
La gestión del tiempo es un pilar fundamental en la ejecución exitosa de cualquier proyecto, especialmente en un Trabajo Fin de Grado (TFG) que requiere una secuencia lógica de actividades y una dedicación constante. La planificación temporal de este proyecto se ha basado en la metodología ágil de \textbf{Sprints de Desarrollo} citados en el apartado ~\ref{Sec.Sprints}, la cual ha permitido una organización estructurada de las tareas, la adaptación a los desafíos y la entrega incremental de funcionalidades, tal como se detalló en la sección de Metodología.

Para la estimación y seguimiento del tiempo, se ha considerado un período de ocho meses, que abarca desde la fase inicial de los fudamentos teóricos hasta la validación y documentación final. La duración de cada sprint fue estimada en dos semanas aproximadamente, permitiendo flexibilidad para la investigación y el desarrollo intensivo de cada módulo. Este enfoque se alinea con prácticas comunes en la planificación de proyectos de ingeniería de software, buscando optimizar el esfuerzo y los recursos disponibles [9].

Para este proyecto, el cronograma se presenta como una secuencia de los sprints ya definidos, cada uno con sus objetivos específicos y una duración estimada.

\subsection{Cronograma del Proyecto}

A continuación, se presenta el cronograma detallado del proyecto, desglosado por los sprints de desarrollo en la figura ~\ref{Fig.Cronograma}. Este cronograma visualiza la secuencia de las actividades principales y la estimación de su duración en meses.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\textwidth]{imagenes/cronograma.png}
  \caption{Diagrama de Gantt del proyecto. Fuente: Elaboración propia.}
  \label{Fig.Cronograma}
\end{figure}


Este cronograma proporciona una guía para la ejecución del proyecto, destacando la superposición de algunos sprints (como la investigación teórica con el diseño inicial, o el preprocesamiento de datos con el desarrollo web) para optimizar el tiempo. La naturaleza iterativa de la metodología ágil permite ajustes en la duración de los sprints según las necesidades emergentes y la complejidad de las tareas [7]. La dedicación principal al proyecto se concentró en la ejecución de estos sprints, culminando con una fase intensiva de integración y pruebas antes de la entrega final.

\subsection{Estimación del Esfuerzo}

La estimación del esfuerzo para el presente Trabajo de Fin de Grado se ha realizado en conformidad con la carga académica establecida para un TFG de 12 créditos ECTS, lo que se traduce en un total de 300 horas de trabajo dedicado. Este volumen de esfuerzo se ha distribuido estratégicamente a lo largo de la duración del proyecto, que abarca desde Noviembre de 2024 hasta finales de Julio de 2025, como se detalla en el cronograma.

Para un perfil de Programador/Investigador Junior, la dedicación requerida para un proyecto de esta envergadura se traduce en una asignación meticulosa de horas a cada fase, cubriendo desde la investigación y el diseño hasta la implementación, depuración y la elaboración de la documentación. Este enfoque es consistente con las prácticas comunes en la planificación de proyectos de software.

El esfuerzo total de 300 horas se ha distribuido entre las distintas fases y actividades del proyecto, alineándose con la metodología de sprints empleada. A continuación, se detalla la asignación de estas horas en las principales etapas:

Análisis y Diseño: Incluye la investigación teórica, la definición de requisitos funcionales y no funcionales, y la conceptualización de la arquitectura inicial del sistema.

Desarrollo e Implementación: Abarca la codificación y construcción de los diversos módulos, como el capturador de tráfico, la herramienta de generación de datasets, el componente de Machine Learning y la interfaz de usuario web.

Pruebas y Validación: Comprende la ejecución de baterías de pruebas, la validación del rendimiento del modelo de Machine Learning y la verificación integral de la funcionalidad del sistema.

Documentación: Incluye la redacción exhaustiva de esta memoria final de grado, la cual culminará a finales de julio de 2025, y la preparación de otros materiales de apoyo.

Esta distribución del esfuerzo, similar a la aplicada en otros proyectos académicos que buscan cumplir con un número de créditos específicos, asegura una asignación proporcional de recursos al trabajo técnico, la investigación y la formalización documental. La planificación detallada y el progreso de estas actividades se visualizan en el Diagrama de Gantt del proyecto, presentado en la ilustración ~\ref{Fig.Cronograma}.

\section{Estructura de la memoria}
La memoria final de este proyecto está organizada en una serie de capítulos que se ajustan a la siguiente estructura:

\begin{itemize}
    \item{Capítulo 2}: Capítulo en el que se discute el estado del arte de sistema de ciberseguridad que apican ML, como diversas líneas de investigación y desarrollo al respecto
    \item{Capítulo 3}: Capítulo en el que se abordan los materiales y métodos empleados en el desarrollo del proyecto, haciéndose especial énfasis a cada una de las funcionalidades desarrolladas tanto en el capturador de paquetes como en el propio modelo de machine learning empleado para la construcción de la aplicación final para usuarios.
    \item{Capítulo 4}: Capítulo en el que se profundiza en el análisis del sistema propuesto. Se especifican los requisitos funcionales y no funcionales que definen las funcionalidades y limitaciones de la aplicación. Además, se presentan diagramas de casos de uso, de secuencia y de flujo, que ilustran a un alto nivel la arquitectura y el comportamiento del sistema.
    \item{Capítulo 5}: Este capítulo documenta el proceso de diseño detallado del sistema, traduciendo los requisitos en una arquitectura concreta. Se presentan los modelos de datos, los diagramas de clases y la estructura modular de la aplicación, definiendo cómo interactúan los componentes del capturador, el modelo de Machine Learning y la interfaz de usuario para construir una solución robusta y escalable.
    \item{Capítulo 6}: En este capítulo se describen los resultados obtenidos tras la implementación y las pruebas del sistema. Se detallan las métricas de rendimiento del modelo de Machine Learning y los hallazgos más relevantes de la evaluación. Se presentan los resultados de la validación del sistema para verificar que cumple con los requisitos iniciales.
    \item{Capítulo 7}: Este apartado presenta las conclusiones del proyecto. Se resumen los logros alcanzados, los desafíos superados y las principales lecciones aprendidas durante el desarrollo. También se discuten las limitaciones del sistema y se proponen posibles mejoras, extensiones y futuras líneas de investigación.
    \item{Capítulo 8}: Este capítulo incluye cualquier material adicional que, aunque no es esencial para la comprensión del texto principal, complementa la información de la memoria. Incluye detalles técnicos adicionales sobre la instalación y configuración del sistema, capturas de pantalla de la interfaz y manual de usuario.
    \item{Capítulo 9}: Este capítulo contiene un glosario de términos clave, acrónimos y abreviaturas utilizados en la memoria. Su propósito es ayudar al lector a comprender la terminología técnica del proyecto, garantizando la claridad y la consistencia a lo largo del documento.
    
\end{itemize}
